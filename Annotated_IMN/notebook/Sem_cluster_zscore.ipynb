{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEMANTIC CLUSTERING OF THE LOCATIONS\n",
    "# (USING THE ZSCORE DATAFRAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm as cm\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import *\n",
    "from sklearn.cluster import KMeans\n",
    "from pandas.plotting import parallel_coordinates\n",
    "import pickle\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import HeatMap\n",
    "import selenium.webdriver\n",
    "\n",
    "plt.rcParams[\"font.family\"] = 'serif'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the parameters to select the correct area and time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = '5'\n",
    "id_area = '11'\n",
    "month =  '9'\n",
    "n_months = '2'\n",
    "week = '0'\n",
    "\n",
    "month_code = month\n",
    "if n_months != \"1\":\n",
    "    for m in range(1, int(n_months)):\n",
    "        month_code += \"_\" + str(int(month)+m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open the dataframe of the location features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../../datasets/out/Traj' + stop + 'min/'\n",
    "file_name_in = 'loc_feat_area'+id_area+'_month'+month_code+'_week'+ week + '_compl_zscore.csv'\n",
    "file_name_out = '_area'+id_area+'_month'+month_code+'_week'+ week + '_zscore'\n",
    "\n",
    "df = pd.read_csv(path+file_name_in)\n",
    "\n",
    "print(\"the number of different vehicles is\", len(df[\"vehicle\"].unique()))\n",
    "print(\"the total number of locations is\", len(df[\"vehicle\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the columns for the vehicle and the location id that are not relevant right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df.copy()\n",
    "df_corr.drop(['vehicle', 'loc_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We plot the distribution of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# draw the distribution of the attributes \n",
    "fig = plt.figure(figsize=(100, 100)) \n",
    "fig_dims = (8, 9)\n",
    "\n",
    "plot_type = [\"line\" for i in df_corr.keys()]\n",
    "plot_type[0:2] = [\"pie\", \"ignore\", \"ignore\"]\n",
    "\n",
    "#plot_type = [\"pie\", \"ignore\", \"ignore\", \"bar\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \n",
    "          #  \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \n",
    "           # \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"line\", \"line\", \"line\", \n",
    "          #   \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\"]\n",
    "plt.rcParams[\"font.size\"] = 13\n",
    "\n",
    "skip = 0\n",
    "\n",
    "for i in range(len(df_corr.keys())-1):\n",
    "    k = df_corr.keys()[i]\n",
    "    t = plot_type[i]\n",
    "    ax = plt.subplot2grid(fig_dims, (int((i-skip)/9), (i-skip)%9))\n",
    "    \n",
    "    if t == \"pie\":\n",
    "        labels = 'Not Regular', 'Regular'\n",
    "        sizes = df_corr[k].value_counts()\n",
    "        explode = (0, 0.05)\n",
    "        c = [\"#97c170\", \"#dde37a\", \"#e1bd66\", \"#EAC435\"]\n",
    "        inside, texts, ltexts = ax.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=50, colors=c)\n",
    "        for i in range(len(texts)):\n",
    "            texts[i].set_fontsize(12)\n",
    "            ltexts[i].set_fontsize(12)\n",
    "        ax.axis('equal') \n",
    "        plt.title(k)\n",
    "        \n",
    "    if t == \"line\":\n",
    "        x = range(0, len(df_corr))\n",
    "        y = sorted(df_corr[k])\n",
    "        plt.plot(x, y, color = '#EAC435', linewidth=2.5)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.xlabel(\"locations\", fontsize=13)\n",
    "        plt.ylabel(k, fontsize=13)\n",
    "        plt.grid(True)\n",
    "        plt.title(k)\n",
    "    \n",
    "    if t == \"bar\":\n",
    "        x = range(0, len(df_corr))\n",
    "        y = sorted(df_corr[k])\n",
    "        _, bins, _ = plt.hist(df_corr[k], 20, color = '#97c170', ec='#FFFFFF')\n",
    "        ax.set_xlabel(k, fontsize=13)\n",
    "        ax.set_ylabel(\"number of locations\", fontsize=13)\n",
    "        plt.title(k)\n",
    "\n",
    "    if t == \"ignore\":\n",
    "        skip += 1\n",
    "        \n",
    "plt.savefig('../../../thesis/images/distribution'+file_name_out+'_minmax.png', format='png', bbox_inches='tight')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw the correlation plot of the individual, collective and geographical features to understand if some attributes are redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color map from purple to orange\n",
    "cmap = cm.get_cmap('PuOr')\n",
    "\n",
    "plt.rcParams[\"font.size\"] = '16'\n",
    "\n",
    "# draw the heatmap first for the individual features, and then for the collective and the geographical\n",
    "# the correlation between the individual and the others are almost none\n",
    "# and this way we work with 2 smaller matrices\n",
    "\n",
    "# individual heatmap\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(40,30)\n",
    "\n",
    "# take only the first part of the dataset\n",
    "correlati = df_corr.iloc[ : , :35].corr()\n",
    "correlati = correlati.round(2)\n",
    "ax = sns.heatmap(correlati, cmap=cmap, vmin = -1, vmax = 1, annot = True,linewidths=.4)\n",
    "\n",
    "# little trick to solve the bug that the heatmap is cut \n",
    "b, t = plt.ylim() # discover the values for bottom and top\n",
    "b += 0.5 # Add 0.5 to the bottom\n",
    "t -= 0.5 # Subtract 0.5 from the top\n",
    "plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.savefig('../../../thesis/images/corr'+file_name_out+'_indiv.png', format='png', bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# collective and geographical heatmap\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(40,30)\n",
    "\n",
    "# take only the second and third part of the dataset\n",
    "correlati = df_corr.iloc[ : , 35:].corr()\n",
    "correlati = correlati.round(2)\n",
    "ax = sns.heatmap(correlati, cmap=cmap, vmin = -1, vmax = 1, annot = True,linewidths=.4)\n",
    "\n",
    "# little trick to solve the bug that the heatmap is cut \n",
    "b, t = plt.ylim() # discover the values for bottom and top\n",
    "b += 0.5 # Add 0.5 to the bottom\n",
    "t -= 0.5 # Subtract 0.5 from the top\n",
    "plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.savefig('../../../thesis/images/corr'+file_name_out+'_coll_geo.png', format='png', bbox_inches='tight')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can remove the attributes that have a high correlation with another\n",
    "\n",
    "In some cases we perform a mean of the correlated columns, in other cases, if the information is just redundant, we just remove the attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.drop([\"support\"], axis=1, inplace=True)\n",
    "\n",
    "avg_stay_weekday = (df_corr[\"avg_stay_weekday_day\"] + df_corr[\"avg_stay_weekday_night\"])/2\n",
    "avg_stay_weekend = (df_corr[\"avg_stay_weekend_day\"] + df_corr[\"avg_stay_weekend_night\"])/2\n",
    "std_stay_weekday = (df_corr[\"std_stay_weekday_day\"] + df_corr[\"std_stay_weekday_night\"])/2\n",
    "std_stay_weekend = (df_corr[\"std_stay_weekend_day\"] + df_corr[\"std_stay_weekend_night\"])/2\n",
    "\n",
    "df_corr = df_corr.assign(avg_stay_weekday=avg_stay_weekday, avg_stay_weekend=avg_stay_weekend,\n",
    "                         std_stay_weekday=std_stay_weekday, std_stay_weekend=std_stay_weekend)\n",
    "\n",
    "df_corr.drop([\"avg_stay_weekday_day\", \"avg_stay_weekday_night\", \"avg_stay_weekend_day\", \"avg_stay_weekend_night\",\n",
    "              \"std_stay_weekday_day\", \"std_stay_weekday_night\", \"std_stay_weekend_day\", \"std_stay_weekend_night\"], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "avg_time_weekday_day = (df_corr[\"avg_leave_weekday_day\"] + df_corr[\"avg_arrive_weekday_day\"])/2\n",
    "avg_time_weekend_day = (df_corr[\"avg_leave_weekend_day\"] + df_corr[\"avg_arrive_weekend_day\"])/2\n",
    "avg_time_weekday_night = (df_corr[\"avg_leave_weekday_night\"] + df_corr[\"avg_arrive_weekday_night\"])/2\n",
    "avg_time_weekend_night = (df_corr[\"avg_leave_weekend_night\"] + df_corr[\"avg_arrive_weekend_night\"])/2\n",
    "\n",
    "df_corr = df_corr.assign(avg_time_weekday_day=avg_time_weekday_day, avg_time_weekend_day=avg_time_weekend_day,\n",
    "                         avg_time_weekday_night=avg_time_weekday_night, avg_time_weekend_night=avg_time_weekend_night)\n",
    "\n",
    "df_corr.drop([\"avg_leave_weekday_day\", \"avg_arrive_weekday_day\", \"avg_leave_weekend_day\", \"avg_arrive_weekend_day\",\n",
    "              \"avg_leave_weekday_night\", \"avg_arrive_weekday_night\", \"avg_leave_weekend_night\", \"avg_arrive_weekend_night\"], axis=1, inplace=True)\n",
    "\n",
    "df_corr.drop([\"avg_leave_mov_duration\", \"avg_arrive_mov_duration\", \"std_leave_mov_duration\", \"std_arrive_mov_duration\"], axis=1, inplace=True)\n",
    "\n",
    "df_corr.drop([\"centrality5K\", \"rev_centrality3\", \"rev_centrality8\", \"rev_centrality10\"], axis=1, inplace=True)\n",
    "\n",
    "# move the geographical features as the last columns of the dataframe\n",
    "categories = [\"gas\", \"parking\", \"pier\", \"hotel\", \"food\", \"leisure\", \"shop\", \"service\", \"supermarket\"]\n",
    "columns_df_g = [\"n_\"+c for c in categories]+[\"k_\"+c for c in categories]+[\"d_\"+c for c in categories]\n",
    "df_corr = df_corr[[c for c in df_corr if c not in columns_df_g] + [c for c in columns_df_g if c in df_corr]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can compute the correlation matrix again after the varible transformation (in this case we draw only one heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix only with the collective and geographic features\n",
    "\n",
    "cmap = cm.get_cmap('PuOr')\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(60, 50)\n",
    "\n",
    "correlati = df_corr.corr()\n",
    "correlati = correlati.round(2)\n",
    "ax = sns.heatmap(correlati, cmap=cmap, vmin = -1, vmax = 1, annot = True,linewidths=.4)\n",
    "\n",
    "b, t = plt.ylim() # discover the values for bottom and top\n",
    "b += 0.5 # Add 0.5 to the bottom\n",
    "t -= 0.5 # Subtract 0.5 from the top\n",
    "plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "plt.savefig('../../../thesis/images/corr'+file_name_out+'_after.png', format='png', bbox_inches='tight')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-MEANS CLUSTERING OF THE FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the sse and the silhouette for k in the range from 2 to 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ DO NOT RUN AGAIN, TOO LONG ############################\n",
    "sse_list = list()\n",
    "sil_list = list()\n",
    "print(\"range 2-10\")\n",
    "for k in range(2,10):\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10, max_iter=100)\n",
    "    kmeans.fit(df_corr)\n",
    "    sse = kmeans.inertia_\n",
    "    sse_list.append(sse)\n",
    "    sil = silhouette_score(df_corr, kmeans.labels_)\n",
    "    sil_list.append(sil)\n",
    "\n",
    "with open(path+\"sse_silouette\"+file_name_out+'.pickle', 'wb') as fp:\n",
    "    pickle.dump(sse_list, fp)\n",
    "    pickle.dump(sil_list, fp)\n",
    "    \n",
    "print(\"range 10-200, step 5\")\n",
    "for k in range(10, 200, 5):\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10, max_iter=100)\n",
    "    kmeans.fit(df_corr)\n",
    "    sse = kmeans.inertia_\n",
    "    sse_list.append(sse)\n",
    "    sil = silhouette_score(df_corr, kmeans.labels_)\n",
    "    sil_list.append(sil)\n",
    "\n",
    "with open(path+\"sse_silouette\"+file_name_out+'.pickle', 'ab') as fp:\n",
    "    pickle.dump(sse_list, fp)\n",
    "    pickle.dump(sil_list, fp)\n",
    "    \n",
    "print(\"range 200-1000, step 100\")\n",
    "for k in range(200, 1000, 100):\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10, max_iter=100)\n",
    "    kmeans.fit(df_corr)\n",
    "    sse = kmeans.inertia_\n",
    "    sse_list.append(sse)\n",
    "    sil = silhouette_score(df_corr, kmeans.labels_)\n",
    "    sil_list.append(sil)\n",
    "    \n",
    "with open(path+\"sse_silouette\"+file_name_out+'.pickle', 'ab') as fp:\n",
    "    pickle.dump(sse_list, fp)\n",
    "    pickle.dump(sil_list, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the pickle files containing the sse and silhouette values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+\"sse_silouette\"+file_name_out+'.pickle', 'rb') as fp:\n",
    "    sse_list1 = pickle.load(fp)\n",
    "    sil_list1 = pickle.load(fp)\n",
    "    \n",
    "    sse_list2 = pickle.load(fp)\n",
    "    sil_list2 = pickle.load(fp)\n",
    "    \n",
    "    sse_list = pickle.load(fp)\n",
    "    sil_list = pickle.load(fp)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw the sse and the silhouette values obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw sse\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(20,12)\n",
    "plt.rcParams[\"font.size\"] = '16'\n",
    "x = list(range(2,10)) + list(range(10, 200, 5)) + list(range(200, 1000, 100))\n",
    "\n",
    "plt.plot(x, sse_list, color = '#A8201A', linewidth=2.5)\n",
    "plt.plot(105, sse_list[27], \"o\", color = '#A8201A', markersize = 10) ## area 11\n",
    "#plt.plot(160, sse_list[38], \"o\", color = '#A8201A', markersize = 10) ## area 2\n",
    "\n",
    "plt.xticks(np.arange(0, 1000, 50))\n",
    "plt.yticks(np.arange(300000, 2000000, 150000)) ## area 11\n",
    "#plt.yticks(np.arange(1000000, 5700000, 250000)) ## area 2\n",
    "plt.xlabel(\"k\", fontsize=19)\n",
    "plt.ylabel(\"sse\", fontsize=19)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig('../../../thesis/images/sse'+file_name_out+'.png', format='png', bbox_inches='tight')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw silhouette\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(20,12)\n",
    "plt.rcParams[\"font.size\"] = '16'\n",
    "x = list(range(2,10)) + list(range(10, 200, 5)) + list(range(200, 1000, 100))\n",
    "\n",
    "plt.plot(x, sil_list, color = '#143642', linewidth=2.5)\n",
    "plt.plot(105, sil_list[27], \"o\", color = '#143642', markersize = 10) ## area 11\n",
    "#plt.plot(160, sil_list[38], \"o\", color = '#143642', markersize = 10) ## area 2\n",
    "\n",
    "plt.xticks(np.arange(0, 1000, 50))\n",
    "plt.yticks(np.arange(0.04, 0.3, 0.02)) ## area 11\n",
    "#plt.yticks(np.arange(0.03, 0.31, 0.02)) ## area 2\n",
    "\n",
    "plt.xlabel(\"k\", fontsize=19)\n",
    "plt.ylabel(\"silhouette\", fontsize=19)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig('../../../thesis/images/sil'+file_name_out+'.png', format='png', bbox_inches='tight')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose the best k for the kmeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_best = 105 ## area 11\n",
    "#k_best = 160 ## area 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run again the kmeans with the k chosen to compute the centroids and the dict from cluster to number of locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ DO NOT RUN AGAIN, TOO LONG ############################\n",
    "kmeans = KMeans(init='k-means++', n_clusters=k_best, n_init=10, max_iter=300, random_state = 123)\n",
    "kmeans.fit(df_corr)\n",
    "\n",
    "# get the centroids\n",
    "centroids_kmeans = kmeans.cluster_centers_\n",
    "labels_kmeans = kmeans.labels_\n",
    "\n",
    "with open(path+\"centroids_kmeans\"+file_name_out+'.pickle', 'wb') as fp:\n",
    "    pickle.dump(centroids_kmeans, fp)\n",
    "    pickle.dump(labels_kmeans, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+\"centroids_kmeans\"+file_name_out+'.pickle', 'rb') as fp:\n",
    "    centroids_kmeans = pickle.load(fp)\n",
    "    labels_kmeans = pickle.load(fp)\n",
    "    \n",
    "hist, bins = np.histogram(labels_kmeans, bins=range(0, len(set(labels_kmeans)) + 1))\n",
    "# dict from cluster id to number of locs in cluster\n",
    "kmeans_cluster_size = dict(zip(bins, hist)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPUTE THE HIERARCHICAL CLUSTERING ON THE CENTROIDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_linkage_matrix(centroids_kmeans):\n",
    "    # compute the distance and the linkage matrix\n",
    "    cmap = cm.gist_rainbow(np.linspace(0.1, 1, 7))\n",
    "    hierarchy.set_link_color_palette([mpl.colors.rgb2hex(rgb[:3]) for rgb in cmap])\n",
    "\n",
    "    # distance matrix\n",
    "    dist_matrix = pdist(centroids_kmeans, metric='euclidean')\n",
    "    # linkage matrix\n",
    "    link_matrix = linkage(dist_matrix, method='ward', metric='euclidean')\n",
    "    return link_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dict from cluster labels to the points in that cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def points_labels_to_clusters(points, labels):\n",
    "    clusters = defaultdict(list)\n",
    "    for i in range(0, len(points)):\n",
    "        clusters[labels[i]].append(points[i])\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute a set of dictionaries useful for computing measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_dict(link_matrix, centroids_kmeans, labels_kmeans, kmeans_cluster_size):\n",
    "    # list of linkage cluster id \n",
    "    linkage_labels = fcluster(link_matrix, cut_dist, 'distance') \n",
    "    # dict from cluster label to the points in it\n",
    "    linkage_clusters = points_labels_to_clusters(np.array(centroids_kmeans), linkage_labels)\n",
    "\n",
    "    # dict from linkage cluster id to number of locations\n",
    "    link_cluster_to_n_location = dict.fromkeys(np.unique(linkage_labels), 0)\n",
    "    # dict from linkage cluster id to number of kmeans clusters\n",
    "    link_cluster_to_n_kcluster = dict.fromkeys(np.unique(linkage_labels), 0)\n",
    "    # dict from kmeans cluster id to linkage cluster id\n",
    "    kcluster_to_link_cluster = dict.fromkeys(np.unique(labels_kmeans), 0)\n",
    "    for i, c in enumerate(linkage_labels):\n",
    "        link_cluster_to_n_location[c] += kmeans_cluster_size[i]\n",
    "        link_cluster_to_n_kcluster[c] += 1\n",
    "        kcluster_to_link_cluster[i] = c\n",
    "        \n",
    "    return linkage_labels, link_cluster_to_n_location, link_cluster_to_n_kcluster, kcluster_to_link_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function to draw the dendrogram of the hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_dendro(k, link_matrix, cut_dist, link_cluster_to_n_location):\n",
    "    # draw the dendrogram of the linkage clustering\n",
    "    fig = plt.figure(figsize=(20, 10)) \n",
    "\n",
    "    res = dendrogram(link_matrix, color_threshold = cut_dist, above_threshold_color = 'grey', no_labels= True)\n",
    "    plt.axhline(y=cut_dist, c='r')\n",
    "    y_ticks_max = round(link_matrix[-1][2])\n",
    "    plt.yticks(np.arange(0, y_ticks_max, y_ticks_max/10), fontsize=16)\n",
    "\n",
    "    cmap = cm.gist_rainbow(np.linspace(0.1, 1, 7))\n",
    "\n",
    "    legend_handles = []\n",
    "    for i in range(1, len(link_cluster_to_n_location)+1):\n",
    "        legend_handles.append(mpatches.Patch(color=cmap[(i-1)%7], label='C'+str(i)+', n_locs ='+str(link_cluster_to_n_location[i])))\n",
    "\n",
    "    plt.legend(handles=legend_handles, loc=1)\n",
    "\n",
    "    plt.savefig('../../../thesis/images/dentro_'+id_area+'_cluster_'+str(k+2)+'.png', format='png', bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataframe containing the kmeans centroids and linkage clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_centroids(centroids_kmeans, kcluster_to_link_cluster):\n",
    "    # create a dataframe containing the kmeans centroids\n",
    "    df_centroids = pd.DataFrame(centroids_kmeans, columns=df_corr.columns)\n",
    "    # add a column containing for each centroids the linkage cluster id\n",
    "    df_centroids[\"link_cluster\"] = kcluster_to_link_cluster.values()\n",
    "\n",
    "    # for each linkage cluster extract all the centroids and compute a mean\n",
    "    link_centroids = []\n",
    "    link_centroids_std = []\n",
    "    for i in range(1, len(link_cluster_to_n_kcluster)+1):\n",
    "        df_i = df_centroids[df_centroids[\"link_cluster\"] == i]\n",
    "        link_centroids.append(list(df_i.mean(axis = 0)))\n",
    "        link_centroids_std.append(list(df_i.std(axis = 0)))\n",
    "        \n",
    "    # create a dataframe containing of each linkage cluster the mean of the centroids in it\n",
    "    df_par = pd.DataFrame(link_centroids, columns=df_centroids.columns)\n",
    "    \n",
    "    return df_centroids, df_par, link_centroids_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw the parallel coordinates of the cluster obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# USING PANDAS LIBRARY\n",
    "def draw_par_coords(df_par, link_cluster_to_n_location, cluster_id):\n",
    "    # draw the parallel coordinates of the linkage clusters\n",
    "    fig = plt.figure(figsize=(35, 12)) \n",
    "\n",
    "    cmap = cm.gist_rainbow(np.linspace(0.1, 1, 7))\n",
    "\n",
    "    parallel_coordinates(df_par, 'link_cluster', color = cmap, linewidth=3, axvlines=True, \\\n",
    "                         axvlines_kwds={\"linewidth\":0.5, \"color\":\"k\"} )\n",
    "    plt.xticks(rotation=90, fontsize=16)\n",
    "    plt.yticks(np.arange(0, 1.01, 0.1), fontsize=16)\n",
    "\n",
    "    legend_handles = []\n",
    "    for i in range(1, len(link_cluster_to_n_location)+1):\n",
    "        legend_handles.append(mpatches.Patch(color=cmap[(i-1)%7], label='C'+str(i)+', n_locs ='+str(link_cluster_to_n_location[i])))\n",
    "\n",
    "    plt.legend(handles=legend_handles, loc=1)\n",
    "    \n",
    "    plt.grid(False)\n",
    "\n",
    "    plt.savefig('../../../thesis/images/parallel_coord_'+id_area+'_cluster_'+str(cluster_id+2)+'.png', format='png', bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### USING ERRORBAR\n",
    "def draw_par_coords_error(df_par, link_centroids_std, link_cluster_to_n_location, cluster_id):\n",
    "    \n",
    "    # draw the parallel coordinates of the linkage clusters\n",
    "    fig, ax = plt.subplots(1,1, figsize=(35, 12)) \n",
    "    #fig = plt.figure() \n",
    "    cmap = cm.gist_rainbow(np.linspace(0.1, 1, 7))\n",
    "    l = len(df_par.keys()[:-1])\n",
    "        \n",
    "    for i, row in df_par.iterrows():\n",
    "        \n",
    "        x = [x + y for x, y in zip(range(l), np.ones(l)*0.05*i)] #df_par.keys()[:-1]\n",
    "        y = row[:-1]\n",
    "        yerr = link_centroids_std[i][:-1]\n",
    "        \n",
    "        (_, caps, _) = plt.errorbar(x, y, yerr=yerr, color=cmap[i], linewidth=5, barsabove=True, \\\n",
    "                            elinewidth=2, uplims=True, lolims=True, label='uplims=True, lolims=True')\n",
    "\n",
    "        for cap in caps:\n",
    "            cap.set_marker(\"_\")\n",
    "            cap.set_markersize(10)\n",
    "            cap.set_markeredgewidth(3)\n",
    "        \n",
    "    x_ticks_labels = df_par.keys()[:-1]\n",
    "    ax.set_xticks(x) # Set number of ticks for x-axis\n",
    "    ax.set_xticklabels(x_ticks_labels, rotation='vertical', fontsize=16) # Set ticks labels for x-axis\n",
    "    plt.yticks(np.arange(-0.2, 1.3, 0.1), fontsize=16)\n",
    "\n",
    "    legend_handles = []\n",
    "    for i in range(1, len(link_cluster_to_n_location)+1):\n",
    "        legend_handles.append('C'+str(i)+', n_locs ='+str(link_cluster_to_n_location[i]))\n",
    "\n",
    "    plt.legend(legend_handles, loc=1)\n",
    "\n",
    "    plt.savefig('../../../thesis/images/parallel_coord_'+id_area+'_cluster_'+str(cluster_id+2)+'_error.png', format='png', bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the clusters splitting recursively according to the dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_matrix = compute_linkage_matrix(centroids_kmeans)\n",
    "\n",
    "for i in range(5):\n",
    "    cut_dist = link_matrix[-i-1][2] - 0.1\n",
    "    \n",
    "    linkage_labels, link_cluster_to_n_location, link_cluster_to_n_kcluster, kcluster_to_link_cluster = clusters_dict(\n",
    "                        link_matrix, centroids_kmeans, labels_kmeans, kmeans_cluster_size)\n",
    "    \n",
    "    draw_dendro(i, link_matrix, cut_dist, link_cluster_to_n_location)    \n",
    "    \n",
    "    df_centroids, df_par, link_centroids_std = create_df_centroids(centroids_kmeans, kcluster_to_link_cluster)\n",
    "    \n",
    "    draw_par_coords_error(df_par, link_centroids_std, link_cluster_to_n_location, i)\n",
    "    draw_par_coords(df_par, link_cluster_to_n_location, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRAW THE HEATMAP OF THE LOCATIONS COMPOSING THE CLUSTERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use selenium to transform a hmtl map into a png image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = selenium.webdriver.PhantomJS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the dataframe not normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataframe not normalized\n",
    "path = '../../../datasets/out/Traj' + stop + 'min/'\n",
    "file_name_in = 'loc_feat_area'+id_area+'_month'+month_code+'_week'+ week + '_complete.csv'\n",
    "\n",
    "df_denorm = pd.read_csv(path+file_name_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_denorm = df_denorm[[\"loc_proto_lat\", \"loc_proto_lon\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the array of linkage cluster label for each location and assign the linkage cluster to the dataset of the locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_cluster = []\n",
    "for kmeans_label in labels_kmeans:\n",
    "    link_cluster.append(kcluster_to_link_cluster[kmeans_label])\n",
    "    \n",
    "df_locs = df_denorm.copy()\n",
    "df_locs[\"link_cluster\"] = link_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract all the points of the locations of a linkage cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array with k (linkage) elements, each contains a list of points in that cluster\n",
    "link_points = []\n",
    "for i in range(1, len(np.unique(link_cluster))+1):\n",
    "    df_i = df_locs[df_locs[\"link_cluster\"] == i]\n",
    "    link_points.append([list(a) for a in zip(df_i[\"loc_proto_lat\"], df_i[\"loc_proto_lon\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw the heatmap for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(link_points)):\n",
    "    cluster_id = i\n",
    "    #m = folium.Map(location=[38, 23], zoom_start=10) ## area 2\n",
    "    m = folium.Map(location=[38, 23.68], zoom_start=12) ## area 11\n",
    "\n",
    "    # Plot it on the map\n",
    "    HeatMap(link_points[cluster_id]).add_to(m)\n",
    "\n",
    "    folium.map.Marker([38.2, 23.68], ## area 11 [38.2, 23.68] ## area 2 [38.8, 22.9]\n",
    "        icon=folium.features.DivIcon(icon_size=(500,40), icon_anchor=(0,0),\n",
    "                                     html='<div style=\"font-size: 56pt\">CLUSTER '+str(cluster_id+1)+'</div>')).add_to(m)\n",
    "    # Display the map\n",
    "    m.save('../../../thesis/images/heatmap_area_'+id_area+'_cluster_'+str(cluster_id+1)+'.html')\n",
    "        \n",
    "    driver.set_window_size(2500, 1800)\n",
    "    driver.get('../../../thesis/images/heatmap_area_'+id_area+'_cluster_'+str(cluster_id+1)+'.html')\n",
    "    driver.save_screenshot('../../../thesis/images/heatmap_area_'+id_area+'_cluster_'+str(cluster_id+1)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
