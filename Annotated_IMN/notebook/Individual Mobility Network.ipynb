{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INDIVIDUAL MOBILITY NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import folium\n",
    "import networkx as nx\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.readwrite import json_graph\n",
    "import pickle\n",
    "from matplotlib import cm as cm\n",
    "import pydot \n",
    "from networkx.drawing.nx_agraph import write_dot\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the parameters to select the correct area and time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = '5'\n",
    "id_area = '2'\n",
    "month =  '9'\n",
    "n_months = '2'\n",
    "week = '0'\n",
    "\n",
    "month_code = month\n",
    "if n_months != \"1\":\n",
    "    for m in range(1, int(n_months)):\n",
    "        month_code += \"_\" + str(int(month)+m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles = ['7410_74050', '15000_131630', '11770_93490'] ## area 2\n",
    "# vehicles = ['14550_128520', '8240_96240', '800_13540'] ## area 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open the dataframe of the location features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../../datasets/out/Traj'+stop+'min/'\n",
    "file_name_in = 'imn_light_area'+id_area+'_month'+month_code+'_week'+ week\n",
    "file_name_out = '_area'+id_area+'_month'+month_code+'_week'+ week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to load the light imn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(path_file):\n",
    "    with open(path_file+'.json', 'r') as f:\n",
    "        file_j = json.load(f)\n",
    "        return file_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the json graph into a networkx friendly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_imn_to_graph(g):\n",
    "    return json_graph.node_link_graph(g, attrs={'link': 'edges', 'source': 'from', 'target': 'to'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translate a value from a range (min_w, max_w) to a range (a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(value, min_w, max_w, a, b):\n",
    "    # Figure out how 'wide' each range is\n",
    "    diff = max_w - min_w\n",
    "    span = b - a\n",
    "\n",
    "    if diff != 0:\n",
    "        # Convert the left range into a 0-1 range (float)\n",
    "        v_scaled = float(value - min_w) / float(diff)\n",
    "\n",
    "        # Convert the 0-1 range into a value in the right range.\n",
    "        return a + (v_scaled * span)\n",
    "    else:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the egdes weight in the range (0.3, 9.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_weights(edges):\n",
    "    w = []\n",
    "    for e in edges:\n",
    "        w.append(e[\"weight\"])\n",
    "\n",
    "    min_w = min(w)\n",
    "    max_w = max(w)\n",
    "    r = float(max_w - min_w)\n",
    "    return [translate(wi, min_w, max_w, 0.3, 10) for wi in w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the nodes size (using the support of the location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nodes_sizes(imn):\n",
    "    nodes_s = []\n",
    "    \n",
    "    loc_feat = imn[\"location_features\"]\n",
    "    locs = list(loc_feat.keys())\n",
    "    for l in locs:\n",
    "        loc_info = loc_feat[str(l)]\n",
    "        if \"loc_support\" in loc_info:\n",
    "            nodes_s.append(loc_info[\"loc_support\"])\n",
    "        else:\n",
    "            nodes_s.append(0)\n",
    "    return nodes_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the nodes size in the range (400, 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_nodes(nodes):\n",
    "    min_n = min(nodes)\n",
    "    max_n = max(nodes)\n",
    "    r = float(max_n - min_n)\n",
    "    return [translate(ni, min_n, max_n, 500, 2000) for ni in nodes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw the imn using networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_imn(v, G, nodes_s, w_scaled, file_name_in):\n",
    "    options = {\n",
    "        'node_color': 'black',\n",
    "        'font_color': \"white\",\n",
    "        'font_size': 13,\n",
    "        'arrows': False,\n",
    "        'font_weight': \"bold\"\n",
    "    }\n",
    "    fig = plt.figure(figsize=(20, 20)) \n",
    "\n",
    "    nx.draw_circular(G, with_labels=True, node_size=nodes_s, width=w_scaled, **options)\n",
    "    plt.savefig('../../../thesis/images/'+file_name_in+'_'+v+'.png', format='png', bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each vehicle compute the imn and store the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_imns(path, file_name_in):\n",
    "    f = open_file(path+file_name_in)\n",
    "    print(\"There are\", len(list(f)), \"vehicles.\")\n",
    "    \n",
    "    for v in vehicles:\n",
    "        g = f[v][\"graph\"]\n",
    "        G = from_imn_to_graph(g)\n",
    "        \n",
    "        w_scaled = scale_weights(g[\"edges\"])\n",
    "        \n",
    "        nodes_order = G.nodes\n",
    "        nodes_weights = scale_nodes(nodes_sizes(f[v]))\n",
    "\n",
    "        nodes_ordered = []\n",
    "        for pos in nodes_order:\n",
    "            nodes_ordered.append(nodes_weights[pos])\n",
    "        \n",
    "        draw_imn(v, G, nodes_ordered, w_scaled, file_name_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute some statistics on the imns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imn_stats(path, file_name_in):\n",
    "    f = open_file(path+file_name_in)\n",
    "    \n",
    "    n_nodes = []\n",
    "    n_edges = []\n",
    "    densities = []\n",
    "    clust_coeff = []\n",
    "    degree_centrality = []\n",
    "    avg_node_degree = []\n",
    "    \n",
    "    for v in list(f):\n",
    "        g = f[v][\"graph\"]\n",
    "        G = from_imn_to_graph(g)\n",
    "        \n",
    "        n_nodes.append(len(G.nodes))\n",
    "        n_edges.append(len(G.edges))\n",
    "        densities.append(nx.density(G))\n",
    "        clust_coeff.append(nx.average_clustering(G))\n",
    "        degree_centrality.append(np.mean(list(nx.degree_centrality(G).values())))\n",
    "        degree_list = [d[1] for d in G.degree]\n",
    "        avg_node_degree.append(np.mean(degree_list))\n",
    "        \n",
    "    print(\"mean(n_nodes)\", np.mean(n_nodes))\n",
    "    print(\"std(n_nodes)\", np.std(n_nodes))\n",
    "    print(\"mean(n_edges)\", np.mean(n_edges))\n",
    "    print(\"std(n_edges)\", np.std(n_edges))\n",
    "    print(\"mean(densities)\", np.mean(densities))\n",
    "    print(\"std(densities)\", np.std(densities))\n",
    "    print(\"mean(clust_coeff)\", np.mean(clust_coeff))\n",
    "    print(\"std(clust_coeff)\", np.std(clust_coeff))\n",
    "    print(\"mean(degree_centrality)\", np.mean(degree_centrality))\n",
    "    print(\"std(degree_centrality)\", np.std(degree_centrality))\n",
    "    print(\"mean(avg_node_degree)\", np.mean(avg_node_degree))\n",
    "    print(\"std(avg_node_degree)\", np.std(avg_node_degree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "store_imns(path, file_name_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imn_stats(path, file_name_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######### TO DRAW SINGLE IMN WITH WHITE NODES\n",
    "\n",
    "# f = open_file(path+file_name_in)\n",
    "# v = \"16020_139450\"\n",
    "# g = f[v][\"graph\"]\n",
    "# G = from_imn_to_graph(g)\n",
    "\n",
    "# w_scaled = scale_weights(g[\"edges\"])\n",
    "\n",
    "# nodes_order = G.nodes\n",
    "# nodes_weights = scale_nodes(nodes_sizes(f[v]))\n",
    "\n",
    "# nodes_ordered = []\n",
    "# for pos in nodes_order:\n",
    "#     nodes_ordered.append(nodes_weights[pos])\n",
    "\n",
    "# draw_imn(v, G, nodes_ordered, w_scaled, file_name_in+\"JJJ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEMANTIC INDIVIDUAL MOBILITY NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a new function to draw the sem imn, so the same graphs but with the nodes colored according to the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sem_imn(v, G, color_nodes, nodes_s, w_scaled, file_name_in):\n",
    "    options = {\n",
    "        'node_color': color_nodes,\n",
    "        'font_color': \"black\",\n",
    "        'font_size': 13,\n",
    "        'arrows': False,\n",
    "        'font_weight': \"bold\"\n",
    "    }\n",
    "    fig = plt.figure(figsize=(20, 20)) \n",
    "\n",
    "    nx.draw_circular(G, with_labels=True, node_size=nodes_s, width=w_scaled, **options)\n",
    "    plt.savefig('../../../thesis/images/sem_imn_'+file_name_out+'_'+v+'.png', format='png', bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the list of cluster ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '_area'+id_area+'_month'+month_code+'_week'+ week\n",
    "\n",
    "with open(path + \"link_cluster\" + file_name + '_log.pickle', 'rb') as fp:\n",
    "    df_link = pickle.load(fp)\n",
    "    link_cluster = df_link[\"link_cluster\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each vehicle extract the list of cluster id, compute the sem imn and store the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def store_sem_imns(path, file_name_in):\n",
    "    f = open_file(path+file_name_in)\n",
    "    for v in vehicles:\n",
    "        g = f[v][\"graph\"]\n",
    "        G = from_imn_to_graph(g)\n",
    "        w_scaled = scale_weights(g[\"edges\"])\n",
    "                \n",
    "        df_link_v = df_link[df_link[\"vehicle\"] == v]        \n",
    "        colors = [\"#525252\", \"#ffff33\", \"#e31a1c\", \"#33a02c\", \"#1f78b4\", \"#e7298a\"]\n",
    "        color_nodes = [colors[c-1] for c in list(df_link_v[\"link_cluster\"])]\n",
    "        \n",
    "        #print(list(df_link_v[\"link_cluster\"]))\n",
    "        print(len(list(df_link_v[\"link_cluster\"])))\n",
    "        nodes_order = G.nodes\n",
    "        nodes_weights = scale_nodes(nodes_sizes(f[v]))\n",
    "        \n",
    "        print(len(nodes_order))\n",
    "        print(len(color_nodes))\n",
    "\n",
    "        colors_ordered = []\n",
    "        nodes_ordered = [] \n",
    "        for pos in nodes_order:\n",
    "            nodes_ordered.append(nodes_weights[pos])\n",
    "            colors_ordered.append(color_nodes[pos])\n",
    "            \n",
    "        print(len(colors_ordered))\n",
    "        print(np.sort(G.nodes))\n",
    "        \n",
    "        draw_sem_imn(v, G, colors_ordered, nodes_ordered, w_scaled, file_name_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### NEW VERSION\n",
    "\n",
    "file_name = '_area'+id_area+'_month'+month_code+'_week'+ week\n",
    "file_name_in = 'loc_feat'+ file_name + '_complete.csv'\n",
    "\n",
    "# read dataframe with location features, extract only the ones we care about\n",
    "df_complete = pd.read_csv(path+file_name_in)\n",
    "df_complete = df_complete[[\"vehicle\", \"loc_id\", \"loc_proto_lat\", \"loc_proto_lon\", \"support\"]]\n",
    "\n",
    "# read the list of linkage clusters for each location\n",
    "with open(path + \"link_cluster\" + file_name + '_log.pickle', 'rb') as fp:\n",
    "    df_link = pickle.load(fp)\n",
    "    link_cluster = df_link[\"link_cluster\"]\n",
    "\n",
    "# store the linkage cluster as new column of the df\n",
    "df_complete = df_complete.assign(link_cluster = link_cluster) \n",
    "\n",
    "\n",
    "\n",
    "def store_sem_imns(path, file_name_in):\n",
    "    \n",
    "    with open(path+\"imn_light\"+file_name_out+'.json', 'rb') as fp:\n",
    "        f = json.load(fp)\n",
    "        for v in vehicles:\n",
    "            \n",
    "            location_nextlocs = f[v][\"location_nextlocs\"]\n",
    "            \n",
    "            g = f[v][\"graph\"]\n",
    "            G = from_imn_to_graph(g)\n",
    "            \n",
    "            print(len(G.nodes)) #1\n",
    "            print(len(G.edges)) #2\n",
    "            w_scaled = scale_weights(g[\"edges\"])\n",
    "\n",
    "            df_link_v = df_link[df_link[\"vehicle\"] == v]        \n",
    "            colors = [\"#525252\", \"#ffff33\", \"#e31a1c\", \"#33a02c\", \"#1f78b4\", \"#e7298a\"]\n",
    "            color_nodes = [colors[c-1] for c in list(df_link_v[\"link_cluster\"])]\n",
    "            \n",
    "            G = nx.DiGraph()\n",
    "            G.add_nodes_from(range(len(df_link_v)))\n",
    "            \n",
    "            for lid1 in location_nextlocs:\n",
    "                for lid2 in location_nextlocs[lid1]:\n",
    "                    G.add_edge(lid1, lid2, weight=location_nextlocs[lid1][lid2])\n",
    "                    \n",
    "            print(len(G.nodes)) #3\n",
    "            print(len(G.edges)) #4\n",
    "            \n",
    "            print(list(df_link_v[\"link_cluster\"]))\n",
    "            print(len(list(df_link_v[\"link_cluster\"])))\n",
    "            nodes_order = G.nodes\n",
    "            nodes_weights = scale_nodes(nodes_sizes(f[v]))\n",
    "\n",
    "            print(len(nodes_order))\n",
    "            print(len(color_nodes))\n",
    "\n",
    "            colors_ordered = []\n",
    "            nodes_ordered = [] \n",
    "            for pos in nodes_order:\n",
    "                nodes_ordered.append(nodes_weights[pos])\n",
    "                colors_ordered.append(color_nodes[pos])\n",
    "\n",
    "            print(len(colors_ordered))\n",
    "            print(np.sort(G.nodes))\n",
    "\n",
    "            draw_sem_imn(v, G, colors_ordered, nodes_ordered, w_scaled, file_name_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_imn(vis_traj = True, tiles='stamentoner', cluster_on=True):\n",
    "    # get dataframe not normalized\n",
    "    file_name = '_area'+id_area+'_month'+month_code+'_week'+ week\n",
    "    file_name_in = 'loc_feat'+ file_name + '_complete.csv'\n",
    "\n",
    "    # read dataframe with location features, extract only the ones we care about\n",
    "    df_complete = pd.read_csv(path+file_name_in)\n",
    "    df_complete = df_complete[[\"vehicle\", \"loc_id\", \"loc_proto_lat\", \"loc_proto_lon\", \"support\"]]\n",
    "\n",
    "    # read the list of linkage clusters for each location\n",
    "    with open(path + \"link_cluster\" + file_name + '_log.pickle', 'rb') as fp:\n",
    "        df_link = pickle.load(fp)\n",
    "        link_cluster = df_link[\"link_cluster\"]\n",
    "\n",
    "    # store the linkage cluster as new column of the df\n",
    "    df_complete = df_complete.assign(link_cluster = link_cluster) \n",
    "\n",
    "    # extract 3 vehicle with different purity levels\n",
    "    vehicles = ['7410_74050', '15000_131630', '11770_93490']\n",
    "\n",
    "    for ix, v in enumerate(vehicles):\n",
    "\n",
    "        # extract the df of locations only of that vehicle\n",
    "        df_v = df_complete[df_complete[\"vehicle\"] == v]\n",
    "        \n",
    "        # read the imn of the vehicle\n",
    "        with open(path+\"imn_light\"+file_name+'.json', 'rb') as fp:\n",
    "            file_j = json.load(fp)\n",
    "            imn_v = file_j[v]\n",
    "        \n",
    "        # extract the dict of nextlocations and the location features\n",
    "        location_nextlocs = imn_v[\"location_nextlocs\"]\n",
    "\n",
    "        # compute a list of points coordinates\n",
    "        points = np.array([[p[0], p[1]] for p in zip(df_v[\"loc_proto_lon\"], df_v[\"loc_proto_lat\"])])\n",
    "\n",
    "        # compute a dict from the location id (as string) to its coordinates\n",
    "        loc_id_string = [str(x) for x in df_v[\"loc_id\"]]\n",
    "        location_prototype = dict(zip(loc_id_string, points))\n",
    "\n",
    "        lat_list = list(df_v[\"loc_proto_lat\"])\n",
    "        lon_list = list(df_v[\"loc_proto_lon\"])\n",
    "        sup_list = list(df_v[\"support\"])\n",
    "\n",
    "        # nero, giallo, rosso, verde, blu, viola\n",
    "        colors = [\"#525252\", \"#ffff33\", \"#e31a1c\", \"#33a02c\", \"#1f78b4\", \"#e7298a\"]\n",
    "        link_cluster = df_v[\"link_cluster\"]\n",
    "        #print(list(link_cluster))\n",
    "        sup_colors = [colors[l-1] for l in link_cluster]\n",
    "        \n",
    "        G = nx.DiGraph()\n",
    "        G.add_nodes_from(range(0, len(lon_list)))\n",
    "        \n",
    "        w = []\n",
    "\n",
    "        for lid1 in location_nextlocs:\n",
    "            for lid2 in location_nextlocs[lid1]:\n",
    "                w.append(location_nextlocs[lid1][lid2])\n",
    "                G.add_edge(lid1, lid2, weight=location_nextlocs[lid1][lid2])\n",
    "                \n",
    "        min_w = min(w)\n",
    "        max_w = max(w)\n",
    "        r = float(max_w - min_w)\n",
    "        w_scaled = [translate(wi, min_w, max_w, 0.3, 10) for wi in w]\n",
    "\n",
    "\n",
    "        for i in range(0, len(lon_list)):\n",
    "            print(i, sup_list[i], sup_colors[i])\n",
    "            \n",
    "        draw_sem_imn(v, G, sup_colors, sup_list, w_scaled, 'imn_light_area'+id_area+'_month'+month_code+'_week'+ week)\n",
    "\n",
    "    return 0\n",
    "visualize_imn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_sem_imns(path, file_name_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUSTERED SEMANTIC INDIVIDUAL MOBILITY NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trasform a color from rgba to hex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_rgba_to_hex(a, b, c):\n",
    "    a = int(a*255)\n",
    "    b = int(b*255)\n",
    "    c = int(c*255)\n",
    "    return '#%02x%02x%02x' % (a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pydot_graph(node_weights, colors, nodes_id, edge_weight, file_name_in, v):\n",
    "    graph = pydot.Dot(graph_type='graph')\n",
    "\n",
    "    graph.set_node_defaults(shape= \"circle\", fixedsize = True, style='filled', color='white')\n",
    "    graph.set_edge_defaults(color=\"black\", dir=\"forward\", arrowhead=\"vee\", arrowsize=\"0.5\", weight=\"50\", minlen=10, mindist=3)\n",
    "\n",
    "    pydot_node_weights = np.divide(node_weights, 5000)\n",
    "\n",
    "    nodes =[]\n",
    "    for i in range(len(nodes_id)):\n",
    "        nodes.append(pydot.Node(str(nodes_id[i]), fillcolor=colors[i], width=str(pydot_node_weights[i])))\n",
    "        graph.add_node(nodes[i])\n",
    "        \n",
    "    from_node_id_to_pos = dict(zip(nodes_id, range(len(nodes_id))))\n",
    "\n",
    "    for e, w in edge_weight.items():\n",
    "        n1_pos = from_node_id_to_pos[e[0]]\n",
    "        n2_pos = from_node_id_to_pos[e[1]]\n",
    "        graph.add_edge(pydot.Edge(nodes[n1_pos], nodes[n2_pos], penwidth=str(w/3), mindist=10))\n",
    "\n",
    "    graph.write_png('../../../thesis/images/clust_sem_imn_'+file_name_out+'_'+v+'.png', prog='circo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_annotated_imn(G, df_link_v, v):\n",
    "    # compute the new list of nodes weights\n",
    "    node_weights = []\n",
    "    # compute the new list of nodes colors\n",
    "    colors = []\n",
    "    # a dict from location id to cluster id\n",
    "    from_loc_to_cluster = dict()\n",
    "    nodes_id = []\n",
    "\n",
    "    # for each cluster take the corresponding df\n",
    "    for i in range(1,7):\n",
    "        df_i = df_link_v[df_link_v[\"link_cluster\"] == i]\n",
    "        # if there's at least a location of that cluster\n",
    "        if len(df_i) != 0:\n",
    "            nodes_id.append(i)\n",
    "            # insert all location in that dict\n",
    "            for i, n in enumerate(df_i[\"loc_id\"]):\n",
    "                from_loc_to_cluster[n] = list(df_i[\"link_cluster\"])[i]\n",
    "                \n",
    "    node_weights = scale_nodes(node_weights)\n",
    "    \n",
    "    # transform the old edges into the new ones\n",
    "    new_edges = []\n",
    "    edge_weight = dict()\n",
    "    \n",
    "    for ed in G.edges.data('weight'):\n",
    "        new_edges.append((from_loc_to_cluster[ed[0]], from_loc_to_cluster[ed[1]]))\n",
    "        first_node = from_loc_to_cluster[ed[0]]\n",
    "        second_node = from_loc_to_cluster[ed[1]]\n",
    "        edge_weight[from_loc_to_cluster[ed[0]], from_loc_to_cluster[ed[1]]] = 0\n",
    "\n",
    "    for ed in G.edges.data('weight'):\n",
    "        new_edges.append((from_loc_to_cluster[ed[0]], from_loc_to_cluster[ed[1]]))\n",
    "        first_node = from_loc_to_cluster[ed[0]]\n",
    "        second_node = from_loc_to_cluster[ed[1]]\n",
    "        edge_weight[from_loc_to_cluster[ed[0]], from_loc_to_cluster[ed[1]]] += ed[2]\n",
    "            \n",
    "    w = edge_weight.values()\n",
    "    \n",
    "    min_w = min(w)\n",
    "    max_w = max(w)\n",
    "    r = float(max_w - min_w)\n",
    "    edge_scaled = [translate(wi, min_w, max_w, 0.3, 10) for wi in w]\n",
    "        \n",
    "    edge_weight = dict(zip(edge_weight.keys(), edge_scaled))\n",
    "        \n",
    "    #draw_pydot_graph(node_weights, colors, nodes_id, edge_weight, file_name_in, v)\n",
    "    \n",
    "    return nodes_id, edge_weight.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each vehicle collapse the nodes according to the clustering, compute the annotated imn and store the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_annotated_imns(path, file_name_in):\n",
    "    nodes_list = []\n",
    "    edge_list = []\n",
    "    \n",
    "    f = open_file(path+file_name_in)\n",
    "    for v in list(f): #vehicles:\n",
    "        # compute the imn graph\n",
    "        g = f[v][\"graph\"]\n",
    "        G = from_imn_to_graph(g)\n",
    "        \n",
    "        df_link_v = df_link[df_link[\"vehicle\"] == v]        \n",
    "        \n",
    "        nodes_order = G.nodes\n",
    "        nodes_weights = scale_nodes(nodes_sizes(f[v]))\n",
    "\n",
    "        nodes_ordered = [] \n",
    "        for pos in nodes_order:\n",
    "            nodes_ordered.append(nodes_weights[pos])\n",
    "            \n",
    "        print(len(df_link_v[\"link_cluster\"]))\n",
    "        print(len(G.nodes))\n",
    "\n",
    "        # take only the data about the vehicle\n",
    "        \n",
    "        df_link_v = df_link_v.assign(graph_node = nodes_ordered) \n",
    "\n",
    "        nodes_id, edges = draw_annotated_imn(G, df_link_v, v)\n",
    "        nodes_list.append(nodes_id)\n",
    "        edge_list.append(edges)\n",
    "        \n",
    "    return nodes_list, edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_nodes(nodes):\n",
    "    min_n = min(nodes)\n",
    "    max_n = max(nodes)\n",
    "    r = float(max_n - min_n)\n",
    "    return [translate(ni, min_n, max_n, 1000, 2500) for ni in nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_list, edge_list = store_annotated_imns(path, file_name_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_list = []\n",
    "edge_list = []\n",
    "\n",
    "# get dataframe not normalized\n",
    "file_name = '_area'+id_area+'_month'+month_code+'_week'+ week\n",
    "file_name_in = 'loc_feat'+ file_name + '_complete.csv'\n",
    "\n",
    "# read dataframe with location features, extract only the ones we care about\n",
    "df_complete = pd.read_csv(path+file_name_in)\n",
    "df_complete = df_complete[[\"vehicle\", \"loc_id\", \"loc_proto_lat\", \"loc_proto_lon\"]]\n",
    "\n",
    "# read the list of linkage clusters for each location\n",
    "with open(path + \"link_cluster\" + file_name + '_log.pickle', 'rb') as fp:\n",
    "    df_link = pickle.load(fp)\n",
    "    link_cluster = df_link[\"link_cluster\"]\n",
    "    \n",
    "# read the imn of the vehicle\n",
    "with open(path+\"imn_light\"+file_name+'.json', 'rb') as fp:\n",
    "    file_j = json.load(fp)\n",
    "\n",
    "# store the linkage cluster as new column of the df\n",
    "df_complete = df_complete.assign(link_cluster = link_cluster) \n",
    "\n",
    "f = open_file(path+'imn_light_area'+id_area+'_month'+month_code+'_week'+ week)\n",
    "for v in list(f):    \n",
    "    g = f[v][\"graph\"]\n",
    "    G1 = from_imn_to_graph(g)\n",
    "\n",
    "    # extract the df of locations only of that vehicle\n",
    "    df_v = df_complete[df_complete[\"vehicle\"] == v]\n",
    "\n",
    "    imn_v = file_j[v]\n",
    "\n",
    "    # extract the dict of nextlocations and the location features\n",
    "    location_nextlocs = imn_v[\"location_nextlocs\"]\n",
    "\n",
    "    lat_list = list(df_v[\"loc_proto_lat\"])\n",
    "    lon_list = list(df_v[\"loc_proto_lon\"])\n",
    "\n",
    "    link_cluster = df_v[\"link_cluster\"]\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(G1.nodes)\n",
    "\n",
    "    for lid1 in location_nextlocs:\n",
    "        for lid2 in location_nextlocs[lid1]:\n",
    "            w.append(location_nextlocs[lid1][lid2])\n",
    "            G.add_edge(int(lid1), int(lid2))\n",
    "                    \n",
    "    # a dict from location id to cluster id\n",
    "    from_loc_to_cluster = dict()\n",
    "    nodes_id = []\n",
    "\n",
    "    # for each cluster take the corresponding df\n",
    "    for i in range(1,7):\n",
    "        df_i = df_v[df_v[\"link_cluster\"] == i]\n",
    "        # if there's at least a location of that cluster\n",
    "        if len(df_i) != 0:\n",
    "            nodes_id.append(i)\n",
    "            # insert all location in that dict\n",
    "            for i, n in enumerate(df_i[\"loc_id\"]):\n",
    "                from_loc_to_cluster[n] = list(df_i[\"link_cluster\"])[i]\n",
    "                            \n",
    "    # transform the old edges into the new ones\n",
    "    new_edges = []\n",
    "    \n",
    "    for ed in G.edges:\n",
    "        new_edges.append((from_loc_to_cluster[ed[0]], from_loc_to_cluster[ed[1]]))\n",
    "    \n",
    "    nodes_list.append(nodes_id)\n",
    "    edge_list.append(new_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = [] #\n",
    "n_edges = []\n",
    "densities = []\n",
    "clust_coeff = []\n",
    "degree_centrality = []\n",
    "avg_node_degree = []\n",
    "\n",
    "for i, v in enumerate(list(f)):\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(nodes_list[i])\n",
    "    G.add_edges_from(edge_list[i])\n",
    "    G = G.to_undirected()\n",
    "\n",
    "    n_nodes.append(len(G.nodes))\n",
    "    n_edges.append(len(G.edges))\n",
    "    densities.append(nx.density(G))\n",
    "    clust_coeff.append(nx.average_clustering(G))\n",
    "    degree_centrality.append(np.mean(list(nx.degree_centrality(G).values())))\n",
    "    degree_list = [d[1] for d in G.degree]\n",
    "    avg_node_degree.append(np.mean(degree_list))\n",
    "\n",
    "print(\"mean(n_nodes)\", np.mean(n_nodes))\n",
    "print(\"std(n_nodes)\", np.std(n_nodes))\n",
    "print(\"mean(n_edges)\", np.mean(n_edges))\n",
    "print(\"std(n_edges)\", np.std(n_edges))\n",
    "print(\"mean(densities)\", np.mean(densities))\n",
    "print(\"std(densities)\", np.std(densities))\n",
    "print(\"mean(avg_node_degree)\", np.mean(avg_node_degree))\n",
    "print(\"std(avg_node_degree)\", np.std(avg_node_degree))\n",
    "print(\"mean(clust_coeff)\", np.mean(clust_coeff))\n",
    "print(\"std(clust_coeff)\", np.std(clust_coeff))\n",
    "print(\"mean(degree_centrality)\", np.mean(degree_centrality))\n",
    "print(\"std(degree_centrality)\", np.std(degree_centrality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imn_annotated_stats(path, file_name_in, nodes_list, edge_list):\n",
    "    \n",
    "    n_nodes = [] #\n",
    "    n_edges = []\n",
    "    densities = []\n",
    "    clust_coeff = []\n",
    "    degree_centrality = []\n",
    "    avg_node_degree = []\n",
    "    \n",
    "    for i, v in enumerate(list(f)):\n",
    "        G = nx.DiGraph()\n",
    "        G.add_nodes_from(nodes_list[i])\n",
    "        G.add_edges_from(edge_list[i])\n",
    "        G = G.to_undirected()\n",
    "\n",
    "        n_nodes.append(len(G.nodes))\n",
    "        n_edges.append(len(G.edges))\n",
    "        densities.append(nx.density(G))\n",
    "        clust_coeff.append(nx.average_clustering(G))\n",
    "        degree_centrality.append(np.mean(list(nx.degree_centrality(G).values())))\n",
    "        degree_list = [d[1] for d in G.degree]\n",
    "        avg_node_degree.append(np.mean(degree_list))\n",
    "        \n",
    "    print(\"mean(n_nodes)\", np.mean(n_nodes))\n",
    "    print(\"std(n_nodes)\", np.std(n_nodes))\n",
    "    print(\"mean(n_edges)\", np.mean(n_edges))\n",
    "    print(\"std(n_edges)\", np.std(n_edges))\n",
    "    print(\"mean(densities)\", np.mean(densities))\n",
    "    print(\"std(densities)\", np.std(densities))\n",
    "    print(\"mean(avg_node_degree)\", np.mean(avg_node_degree))\n",
    "    print(\"std(avg_node_degree)\", np.std(avg_node_degree))\n",
    "    print(\"mean(clust_coeff)\", np.mean(clust_coeff))\n",
    "    print(\"std(clust_coeff)\", np.std(clust_coeff))\n",
    "    print(\"mean(degree_centrality)\", np.mean(degree_centrality))\n",
    "    print(\"std(degree_centrality)\", np.std(degree_centrality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imn_annotated_stats(path, file_name_in, nodes_list, edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\begin{table}[t]\n",
    "    \\centering\n",
    "    \\begin{tabular}{|c|c|c|}\n",
    "    \\hline\n",
    "    \\textbf{Measure} & \\textbf{Inter-regional Area} & \\textbf{Urban Area} \\\\  \n",
    "    \\hline\n",
    "    Nodes &  $\\pm$  & 5.06 $\\pm$ 1.02 \\\\ \n",
    "    Edges &  $\\pm$  & 13.31 $\\pm$ 4.56 \\\\ \n",
    "    Density &  $\\pm$  & 1.28 $\\pm$ 0.32 \\\\ \n",
    "    Degree &  $\\pm$  & 5.10 $\\pm$ 1.10 \\\\\n",
    "    Clus. Coef. &  $\\pm$  & 0.85 $\\pm$ 0.20 \\\\\n",
    "    Degree Centrality &  $\\pm$ 0.31 & 1.29 $\\pm$ 0.29 \\\\\n",
    "    \\hline\n",
    "    \\end{tabular}\n",
    "\\end{table}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
