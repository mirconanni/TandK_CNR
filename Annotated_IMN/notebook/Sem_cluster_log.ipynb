{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEMANTIC CLUSTERING OF THE LOCATIONS\n",
    "# (USING THE LOG DATAFRAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm as cm\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import *\n",
    "from sklearn.cluster import KMeans\n",
    "from pandas.plotting import parallel_coordinates\n",
    "import pickle\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import HeatMap\n",
    "import selenium.webdriver\n",
    "import json\n",
    "from sklearn import preprocessing\n",
    "\n",
    "plt.rcParams[\"font.family\"] = 'serif'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the parameters to select the correct area and time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = '5'\n",
    "id_area = '2'\n",
    "month =  '9'\n",
    "n_months = '2'\n",
    "week = '0'\n",
    "\n",
    "month_code = month\n",
    "if n_months != \"1\":\n",
    "    for m in range(1, int(n_months)):\n",
    "        month_code += \"_\" + str(int(month)+m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open the dataframe of the location features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../../datasets/out/Traj' + stop + 'min/'\n",
    "file_name_in = 'loc_feat_area'+id_area+'_month'+month_code+'_week'+ week + '_compl_log_norm.csv'\n",
    "file_name_out = '_area'+id_area+'_month'+month_code+'_week'+ week + '_log'\n",
    "\n",
    "df = pd.read_csv(path+file_name_in)\n",
    "\n",
    "print(\"the number of different vehicles is\", len(df[\"vehicle\"].unique()))\n",
    "tot_loc = len(df[\"vehicle\"])\n",
    "print(\"the total number of locations is\", tot_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the columns for the vehicle and the location id that are not relevant right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df.copy()\n",
    "df_corr.drop(['vehicle', 'loc_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We plot the distribution of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# draw the distribution of the attributes \n",
    "fig = plt.figure(figsize=(100, 100)) \n",
    "fig_dims = (8, 9)\n",
    "\n",
    "plot_type = [\"line\" for i in df_corr.keys()]\n",
    "plot_type[0:2] = [\"pie\", \"ignore\", \"ignore\"]\n",
    "\n",
    "#plot_type = [\"pie\", \"ignore\", \"ignore\", \"bar\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \n",
    "          #  \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \n",
    "           # \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"line\", \"line\", \"line\", \n",
    "          #   \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\"]\n",
    "plt.rcParams[\"font.size\"] = 13\n",
    "\n",
    "skip = 0\n",
    "\n",
    "for i in range(len(df_corr.keys())-1):\n",
    "    k = df_corr.keys()[i]\n",
    "    t = plot_type[i]\n",
    "    ax = plt.subplot2grid(fig_dims, (int((i-skip)/9), (i-skip)%9))\n",
    "    \n",
    "    if t == \"pie\":\n",
    "        labels = 'Not Regular', 'Regular'\n",
    "        sizes = df_corr[k].value_counts()\n",
    "        explode = (0, 0.05)\n",
    "        c = [\"#97c170\", \"#dde37a\", \"#e1bd66\", \"#EAC435\"]\n",
    "        inside, texts, ltexts = ax.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=50, colors=c)\n",
    "        for i in range(len(texts)):\n",
    "            texts[i].set_fontsize(12)\n",
    "            ltexts[i].set_fontsize(12)\n",
    "        ax.axis('equal') \n",
    "        plt.title(k)\n",
    "        \n",
    "    if t == \"line\":\n",
    "        x = range(0, len(df_corr))\n",
    "        y = sorted(df_corr[k])\n",
    "        plt.plot(x, y, color = '#EAC435', linewidth=2.5)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.xlabel(\"locations\", fontsize=13)\n",
    "        plt.ylabel(k, fontsize=13)\n",
    "        plt.grid(True)\n",
    "        plt.title(k)\n",
    "    \n",
    "    if t == \"bar\":\n",
    "        x = range(0, len(df_corr))\n",
    "        y = sorted(df_corr[k])\n",
    "        _, bins, _ = plt.hist(df_corr[k], 20, color = '#97c170', ec='#FFFFFF')\n",
    "        ax.set_xlabel(k, fontsize=13)\n",
    "        ax.set_ylabel(\"number of locations\", fontsize=13)\n",
    "        plt.title(k)\n",
    "\n",
    "    if t == \"ignore\":\n",
    "        skip += 1\n",
    "        \n",
    "plt.savefig('../../../thesis/images/distribution'+file_name_out+'_minmax.png', format='png', bbox_inches='tight')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw the correlation plot of the individual, collective and geographical features to understand if some attributes are redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color map from purple to orange\n",
    "cmap = cm.get_cmap('PuOr')\n",
    "\n",
    "plt.rcParams[\"font.size\"] = '16'\n",
    "\n",
    "# draw the heatmap first for the individual features, and then for the collective and the geographical\n",
    "# the correlation between the individual and the others are almost none\n",
    "# and this way we work with 2 smaller matrices\n",
    "\n",
    "# individual heatmap\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(40,30)\n",
    "\n",
    "# take only the first part of the dataset\n",
    "correlati = df_corr.iloc[ : , :35].corr()\n",
    "correlati = correlati.round(2)\n",
    "ax = sns.heatmap(correlati, cmap=cmap, vmin = -1, vmax = 1, annot = True,linewidths=.4)\n",
    "\n",
    "# little trick to solve the bug that the heatmap is cut \n",
    "b, t = plt.ylim() # discover the values for bottom and top\n",
    "b += 0.5 # Add 0.5 to the bottom\n",
    "t -= 0.5 # Subtract 0.5 from the top\n",
    "plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.savefig('../../../thesis/images/corr'+file_name_out+'_indiv.png', format='png', bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# collective and geographical heatmap\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(40,30)\n",
    "\n",
    "# take only the second and third part of the dataset\n",
    "correlati = df_corr.iloc[ : , 35:].corr()\n",
    "correlati = correlati.round(2)\n",
    "ax = sns.heatmap(correlati, cmap=cmap, vmin = -1, vmax = 1, annot = True,linewidths=.4)\n",
    "\n",
    "# little trick to solve the bug that the heatmap is cut \n",
    "b, t = plt.ylim() # discover the values for bottom and top\n",
    "b += 0.5 # Add 0.5 to the bottom\n",
    "t -= 0.5 # Subtract 0.5 from the top\n",
    "plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.savefig('../../../thesis/images/corr'+file_name_out+'_coll_geo.png', format='png', bbox_inches='tight')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can remove the attributes that have a high correlation with another\n",
    "\n",
    "In some cases we perform a mean of the correlated columns, in other cases, if the information is just redundant, we just remove the attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.drop([\"support\"], axis=1, inplace=True)\n",
    "\n",
    "avg_stay_weekday = (df_corr[\"avg_stay_weekday_day\"] + df_corr[\"avg_stay_weekday_night\"])/2\n",
    "avg_stay_weekend = (df_corr[\"avg_stay_weekend_day\"] + df_corr[\"avg_stay_weekend_night\"])/2\n",
    "std_stay_weekday = (df_corr[\"std_stay_weekday_day\"] + df_corr[\"std_stay_weekday_night\"])/2\n",
    "std_stay_weekend = (df_corr[\"std_stay_weekend_day\"] + df_corr[\"std_stay_weekend_night\"])/2\n",
    "\n",
    "df_corr = df_corr.assign(avg_stay_weekday=avg_stay_weekday, avg_stay_weekend=avg_stay_weekend,\n",
    "                         std_stay_weekday=std_stay_weekday, std_stay_weekend=std_stay_weekend)\n",
    "\n",
    "df_corr.drop([\"avg_stay_weekday_day\", \"avg_stay_weekday_night\", \"avg_stay_weekend_day\", \"avg_stay_weekend_night\",\n",
    "              \"std_stay_weekday_day\", \"std_stay_weekday_night\", \"std_stay_weekend_day\", \"std_stay_weekend_night\"], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "avg_time_weekday_day = (df_corr[\"avg_leave_weekday_day\"] + df_corr[\"avg_arrive_weekday_day\"])/2\n",
    "avg_time_weekend_day = (df_corr[\"avg_leave_weekend_day\"] + df_corr[\"avg_arrive_weekend_day\"])/2\n",
    "avg_time_weekday_night = (df_corr[\"avg_leave_weekday_night\"] + df_corr[\"avg_arrive_weekday_night\"])/2\n",
    "avg_time_weekend_night = (df_corr[\"avg_leave_weekend_night\"] + df_corr[\"avg_arrive_weekend_night\"])/2\n",
    "\n",
    "df_corr = df_corr.assign(avg_time_weekday_day=avg_time_weekday_day, avg_time_weekend_day=avg_time_weekend_day,\n",
    "                         avg_time_weekday_night=avg_time_weekday_night, avg_time_weekend_night=avg_time_weekend_night)\n",
    "\n",
    "df_corr.drop([\"avg_leave_weekday_day\", \"avg_arrive_weekday_day\", \"avg_leave_weekend_day\", \"avg_arrive_weekend_day\",\n",
    "              \"avg_leave_weekday_night\", \"avg_arrive_weekday_night\", \"avg_leave_weekend_night\", \"avg_arrive_weekend_night\"], axis=1, inplace=True)\n",
    "\n",
    "df_corr.drop([\"avg_leave_mov_duration\", \"avg_arrive_mov_duration\", \"std_leave_mov_duration\", \"std_arrive_mov_duration\"], axis=1, inplace=True)\n",
    "\n",
    "df_corr.drop([\"centrality5K\", \"rev_centrality3\", \"rev_centrality8\", \"rev_centrality10\"], axis=1, inplace=True)\n",
    "\n",
    "# move the collective features as the last columns of the dataframe\n",
    "columns_df_c = [\"exclusivity\", \"centrality1K\", \"centrality15K\", \"rev_centrality1\", \"rev_centrality5\", \"rev_centrality20\"]\n",
    "df_corr = df_corr[[c for c in df_corr if c not in columns_df_c] + [c for c in columns_df_c if c in df_corr]]\n",
    "\n",
    "# move the geographical features as the last columns of the dataframe\n",
    "categories = [\"gas\", \"parking\", \"pier\", \"hotel\", \"food\", \"leisure\", \"shop\", \"service\", \"supermarket\"]\n",
    "columns_df_g = [\"n_\"+c for c in categories]+[\"k_\"+c for c in categories]+[\"d_\"+c for c in categories]\n",
    "df_corr = df_corr[[c for c in df_corr if c not in columns_df_g] + [c for c in columns_df_g if c in df_corr]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can compute the correlation matrix again after the varible transformation (in this case we draw only one heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix only with the collective and geographic features\n",
    "\n",
    "cmap = cm.get_cmap('PuOr')\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(60, 50)\n",
    "\n",
    "correlati = df_corr.corr()\n",
    "correlati = correlati.round(2)\n",
    "ax = sns.heatmap(correlati, cmap=cmap, vmin = -1, vmax = 1, annot = True,linewidths=.4)\n",
    "\n",
    "b, t = plt.ylim() # discover the values for bottom and top\n",
    "b += 0.5 # Add 0.5 to the bottom\n",
    "t -= 0.5 # Subtract 0.5 from the top\n",
    "plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "plt.savefig('../../../thesis/images/corr'+file_name_out+'_after.png', format='png', bbox_inches='tight')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-MEANS CLUSTERING OF THE FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the sse and the silhouette for k in the range from 2 to 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ DO NOT RUN AGAIN, TOO LONG ############################\n",
    "sse_list = list()\n",
    "# sil_list = list()\n",
    "print(\"range 2-10, step 2\") #4run\n",
    "for k in range(2, 10, 2):\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10, max_iter=100)\n",
    "    kmeans.fit(df_corr)\n",
    "    sse = kmeans.inertia_\n",
    "    sse_list.append(sse)\n",
    "#     sil = silhouette_score(df_corr, kmeans.labels_)\n",
    "#     sil_list.append(sil)\n",
    "\n",
    "with open(path+\"sse\"+file_name_out+'.pickle', 'wb') as fp:\n",
    "    pickle.dump(sse_list, fp)\n",
    "#     pickle.dump(sil_list, fp)\n",
    "    \n",
    "print(\"range 10-50, step 5\") #8run\n",
    "for k in range(10, 50, 5):\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10, max_iter=100)\n",
    "    kmeans.fit(df_corr)\n",
    "    sse = kmeans.inertia_\n",
    "    sse_list.append(sse)\n",
    "#     sil = silhouette_score(df_corr, kmeans.labels_)\n",
    "#     sil_list.append(sil)\n",
    "\n",
    "with open(path+\"sse\"+file_name_out+'.pickle', 'ab') as fp:\n",
    "    pickle.dump(sse_list, fp)\n",
    "#     pickle.dump(sil_list, fp)\n",
    "\n",
    "print(\"range 50-200, step 15\") #10run\n",
    "for k in range(50, 200, 15):\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10, max_iter=100)\n",
    "    kmeans.fit(df_corr)\n",
    "    sse = kmeans.inertia_\n",
    "    sse_list.append(sse)\n",
    "#     sil = silhouette_score(df_corr, kmeans.labels_)\n",
    "#     sil_list.append(sil)\n",
    "\n",
    "with open(path+\"sse\"+file_name_out+'.pickle', 'ab') as fp:\n",
    "    pickle.dump(sse_list, fp)\n",
    "#     pickle.dump(sil_list, fp)\n",
    "    \n",
    "print(\"range 200-1000, step 100\") #4run\n",
    "for k in range(200, 1000, 100):\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10, max_iter=100)\n",
    "    kmeans.fit(df_corr)\n",
    "    sse = kmeans.inertia_\n",
    "    sse_list.append(sse)\n",
    "#     sil = silhouette_score(df_corr, kmeans.labels_)\n",
    "#     sil_list.append(sil)\n",
    "    \n",
    "with open(path+\"sse\"+file_name_out+'.pickle', 'ab') as fp:\n",
    "    pickle.dump(sse_list, fp)\n",
    "#     pickle.dump(sil_list, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the pickle files containing the sse and silhouette values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+\"sse\"+file_name_out+'.pickle', 'rb') as fp:\n",
    "    sse_list1 = pickle.load(fp)\n",
    "#     sil_list1 = pickle.load(fp)\n",
    "    \n",
    "    sse_list2 = pickle.load(fp)\n",
    "#     sil_list2 = pickle.load(fp)\n",
    "\n",
    "    sse_list3 = pickle.load(fp)\n",
    "#     sil_list2 = pickle.load(fp)\n",
    "\n",
    "    sse_list = pickle.load(fp)\n",
    "#     sil_list = pickle.load(fp)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw the sse and the silhouette values obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw sse\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(6,3)\n",
    "plt.rcParams[\"font.size\"] = '16'\n",
    "x = list(range(2, 10, 2)) + list(range(10, 50, 5)) + list(range(50, 200, 15)) + list(range(200, 1000, 100))\n",
    "\n",
    "# plt.plot(x, sse_list, color = '#0c2c84', linewidth=4)## area 11\n",
    "# plt.plot(140, sse_list[18], \"o\", color = '#0c2c84', markersize = 10) ## area 11\n",
    "plt.plot(x, sse_list, color = '#A8201A', linewidth=4) ## area 2\n",
    "plt.plot(155, sse_list[19], \"o\", color = '#A8201A', markersize = 10) ## area 2\n",
    "\n",
    "# plt.yticks(np.arange(20000, 61000, 20000), [\"20k\", \"40k\", \"60k\"]) ## area 11\n",
    "plt.yticks(np.arange(100000, 210000, 50000), [\"100k\", \"150k\", \"200k\"]) ## area 2\n",
    "plt.xlabel(\"number of clusters (k)\", fontsize=19)\n",
    "plt.ylabel(\"sse\", fontsize=19)\n",
    "plt.title(\"Inter-regional Area\")## area 2\n",
    "# plt.title(\"Urban Area\") ## area 11\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig('../../../thesis/images/sse'+file_name_out+'.png', format='png', bbox_inches='tight')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # draw silhouette\n",
    "# fig = plt.figure()\n",
    "# fig.set_size_inches(20,12)\n",
    "# plt.rcParams[\"font.size\"] = '16'\n",
    "# x = list(range(2,10)) + list(range(10, 200, 5)) + list(range(200, 1000, 100))\n",
    "\n",
    "# plt.plot(x, sil_list, color = '#143642', linewidth=2.5)\n",
    "# # plt.plot(140, sil_list[34], \"o\", color = '#143642', markersize = 10) ## area 11\n",
    "# plt.plot(155, sil_list[37], \"o\", color = '#143642', markersize = 10) ## area 2\n",
    "\n",
    "# # plt.xticks(np.arange(0, 1000, 50)) ## area 11\n",
    "# # plt.yticks(np.arange(0.06, 0.24, 0.01)) ## area 11\n",
    "# plt.xticks(np.arange(0, 1000, 50)) ## area 2\n",
    "# plt.yticks(np.arange(0.06, 0.22, 0.01)) ## area 2\n",
    "\n",
    "# plt.xlabel(\"k\", fontsize=19)\n",
    "# plt.ylabel(\"silhouette\", fontsize=19)\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.savefig('../../../thesis/images/sil'+file_name_out+'.png', format='png', bbox_inches='tight')\n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose the best k for the kmeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k_best = 140 ## area 11\n",
    "k_best = 155 ## area 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run again the kmeans with the k chosen to compute the centroids and the dict from cluster to number of locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ DO NOT RUN AGAIN, TOO LONG ############################\n",
    "kmeans = KMeans(init='k-means++', n_clusters=k_best, n_init=10, max_iter=300, random_state = 123)\n",
    "kmeans.fit(df_corr)\n",
    "\n",
    "# get the centroids\n",
    "centroids_kmeans = kmeans.cluster_centers_\n",
    "labels_kmeans = kmeans.labels_\n",
    "\n",
    "with open(path+\"centroids_kmeans\"+file_name_out+'.pickle', 'wb') as fp:\n",
    "    pickle.dump(centroids_kmeans, fp)\n",
    "    pickle.dump(labels_kmeans, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area 11\n",
    "print(\"sse\", kmeans.inertia_)\n",
    "print(\"sil\", silhouette_score(df_corr, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area 2\n",
    "print(\"sse\", kmeans.inertia_)\n",
    "print(\"sil\", silhouette_score(df_corr, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+\"centroids_kmeans\"+file_name_out+'.pickle', 'rb') as fp:\n",
    "    centroids_kmeans = pickle.load(fp)\n",
    "    labels_kmeans = pickle.load(fp)\n",
    "    \n",
    "hist, bins = np.histogram(labels_kmeans, bins=range(0, len(set(labels_kmeans)) + 1))\n",
    "# dict from cluster id to number of locs in cluster\n",
    "kmeans_cluster_size = dict(zip(bins, hist)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPUTE THE HIERARCHICAL CLUSTERING ON THE CENTROIDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_linkage_matrix(centroids_kmeans):\n",
    "    # distance matrix\n",
    "    dist_matrix = pdist(centroids_kmeans, metric='euclidean')\n",
    "    # linkage matrix\n",
    "    link_matrix = linkage(dist_matrix, method='ward', metric='euclidean')\n",
    "    return link_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dict from cluster labels to the points in that cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def points_labels_to_clusters(points, labels):\n",
    "    clusters = defaultdict(list)\n",
    "    for i in range(0, len(points)):\n",
    "        clusters[labels[i]].append(points[i])\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute a set of dictionaries useful for computing measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_dict(link_matrix, centroids_kmeans, labels_kmeans, kmeans_cluster_size):\n",
    "    # list of linkage cluster id \n",
    "    linkage_labels = fcluster(link_matrix, cut_dist, 'distance') \n",
    "    # dict from cluster label to the points in it\n",
    "    linkage_clusters = points_labels_to_clusters(np.array(centroids_kmeans), linkage_labels)\n",
    "\n",
    "    # dict from linkage cluster id to number of locations\n",
    "    link_cluster_to_n_location = dict.fromkeys(np.unique(linkage_labels), 0)\n",
    "    # dict from linkage cluster id to number of kmeans clusters\n",
    "    link_cluster_to_n_kcluster = dict.fromkeys(np.unique(linkage_labels), 0)\n",
    "    # dict from kmeans cluster id to linkage cluster id\n",
    "    kcluster_to_link_cluster = dict.fromkeys(np.unique(labels_kmeans), 0)\n",
    "    for i, c in enumerate(linkage_labels):\n",
    "        link_cluster_to_n_location[c] += kmeans_cluster_size[i]\n",
    "        link_cluster_to_n_kcluster[c] += 1\n",
    "        kcluster_to_link_cluster[i] = c\n",
    "        \n",
    "    return linkage_labels, link_cluster_to_n_location, link_cluster_to_n_kcluster, kcluster_to_link_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function to draw the dendrogram of the hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_dendro(cluster_id, link_matrix, cut_dist, link_cluster_to_n_location, colors, partial_cluster):\n",
    "    # draw the dendrogram of the linkage clustering\n",
    "    fig = plt.figure(figsize=(11, 4)) \n",
    "    \n",
    "    mpl.rcParams['lines.linewidth'] = 2\n",
    "    plt.rcParams[\"font.size\"] = 16\n",
    "    \n",
    "    hierarchy.set_link_color_palette(colors)\n",
    "\n",
    "    res = dendrogram(link_matrix, color_threshold = cut_dist, above_threshold_color = 'grey', no_labels= True)\n",
    "    plt.axhline(y=cut_dist, c='r')\n",
    "    y_ticks_max = round(link_matrix[-1][2])\n",
    "    plt.yticks(np.arange(0, y_ticks_max+1, y_ticks_max/5), fontsize=18)\n",
    "    plt.xlabel(\"K-Means centroids\", fontsize=18)\n",
    "    plt.ylabel(\"cluster distance\", fontsize=18)\n",
    "\n",
    "    legend_handles = []\n",
    "    for i in range(len(partial_cluster)):\n",
    "        p = int(round((link_cluster_to_n_location[i+1] * 100) /tot_loc))\n",
    "        legend_handles.append(mpatches.Patch(color=colors[i], label=f\"$C_{i+1} ({p}\\%)$\"))\n",
    "\n",
    "    legend = plt.legend(handles=legend_handles, loc=1, ncol=2)\n",
    "    \n",
    "#     plt.title(\"Inter-regional Area\") ## area 2\n",
    "    plt.title(\"Urban Area\") ## area 11\n",
    "\n",
    "    plt.savefig('../../../thesis/images/dentro_'+id_area+'_cluster_'+str(cluster_id+2)+'.png', format='png', bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataframe containing the kmeans centroids and linkage clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_centroids(centroids_kmeans, kcluster_to_link_cluster):\n",
    "    # create a dataframe containing the kmeans centroids\n",
    "    df_centroids = pd.DataFrame(centroids_kmeans, columns=df_corr.columns)\n",
    "    # add a column containing for each centroids the linkage cluster id\n",
    "    df_centroids[\"link_cluster\"] = kcluster_to_link_cluster.values()\n",
    "\n",
    "    # for each linkage cluster extract all the centroids and compute a mean\n",
    "    link_centroids = []\n",
    "    link_centroids_std = []\n",
    "    for i in range(1, len(link_cluster_to_n_kcluster)+1):\n",
    "        df_i = df_centroids[df_centroids[\"link_cluster\"] == i]\n",
    "        link_centroids.append(list(df_i.mean(axis = 0)))\n",
    "        link_centroids_std.append(list(df_i.std(axis = 0)))\n",
    "        \n",
    "    # create a dataframe containing of each linkage cluster the mean of the centroids in it\n",
    "    df_par = pd.DataFrame(link_centroids, columns=df_centroids.columns)\n",
    "    \n",
    "    return df_centroids, df_par, link_centroids_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw the parallel coordinates of the cluster obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################# USING PANDAS LIBRARY ################ OLD VERSION\n",
    "# def draw_par_coords(df_par, link_cluster_to_n_location, cluster_id):\n",
    "#     # draw the parallel coordinates of the linkage clusters\n",
    "#     fig = plt.figure(figsize=(35, 12)) \n",
    "\n",
    "#     cmap = cm.gist_rainbow(np.linspace(0.1, 1, 7))\n",
    "\n",
    "#     parallel_coordinates(df_par, 'link_cluster', color = cmap, linewidth=3, axvlines=True, \\\n",
    "#                          axvlines_kwds={\"linewidth\":0.5, \"color\":\"k\"} )\n",
    "#     plt.xticks(rotation=90, fontsize=16)\n",
    "#     plt.yticks(np.arange(0, 1.01, 0.1), fontsize=16)\n",
    "\n",
    "#     legend_handles = []\n",
    "#     for i in range(1, len(link_cluster_to_n_location)+1):\n",
    "#         legend_handles.append(mpatches.Patch(color=cmap[(i-1)%7], label='C'+str(i)+', n_locs ='+str(link_cluster_to_n_location[i])))\n",
    "\n",
    "#     plt.legend(handles=legend_handles, loc=1)\n",
    "    \n",
    "#     plt.grid(False)\n",
    "\n",
    "#     plt.savefig('../../../thesis/images/parallel_coord_'+id_area+'_cluster_'+str(cluster_id+2)+'.png', format='png', bbox_inches='tight')\n",
    "#     plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############### USING ERRORBAR\n",
    "# def draw_par_coords_error(df_par, link_centroids_std, link_cluster_to_n_location, cluster_id):\n",
    "    \n",
    "#     # draw the parallel coordinates of the linkage clusters\n",
    "#     fig, ax = plt.subplots(1,1, figsize=(35, 12)) \n",
    "#     #fig = plt.figure() \n",
    "#     cmap = cm.gist_rainbow(np.linspace(0.1, 1, 7))\n",
    "#     l = len(df_par.keys()[:-1])\n",
    "        \n",
    "#     for i, row in df_par.iterrows():\n",
    "        \n",
    "#         x = [x + y for x, y in zip(range(l), np.ones(l)*0.05*i)] #df_par.keys()[:-1]\n",
    "#         y = row[:-1]\n",
    "#         yerr = link_centroids_std[i][:-1]\n",
    "        \n",
    "#         (_, caps, _) = plt.errorbar(x, y, yerr=yerr, color=cmap[i], linewidth=5, barsabove=True, \\\n",
    "#                             elinewidth=2, uplims=True, lolims=True, label='uplims=True, lolims=True')\n",
    "\n",
    "#         for cap in caps:\n",
    "#             cap.set_marker(\"_\")\n",
    "#             cap.set_markersize(10)\n",
    "#             cap.set_markeredgewidth(3)\n",
    "        \n",
    "#     x_ticks_labels = df_par.keys()[:-1]\n",
    "#     ax.set_xticks(x) # Set number of ticks for x-axis\n",
    "#     ax.set_xticklabels(x_ticks_labels, rotation='vertical', fontsize=16) # Set ticks labels for x-axis\n",
    "#     plt.yticks(np.arange(-0.2, 1.3, 0.1), fontsize=16)\n",
    "\n",
    "#     legend_handles = []\n",
    "#     for i in range(1, len(link_cluster_to_n_location)+1):\n",
    "#         legend_handles.append('C'+str(i)+', n_locs ='+str(link_cluster_to_n_location[i]))\n",
    "\n",
    "#     plt.legend(legend_handles, loc=1)\n",
    "\n",
    "#     plt.savefig('../../../thesis/images/parallel_coord_'+id_area+'_cluster_'+str(cluster_id+2)+'_error.png', format='png', bbox_inches='tight')\n",
    "#     plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_feat(df_par):\n",
    "    df_small_par = df_par.drop(['std_leave_mov_length', 'std_arrive_mov_length', 'std_stay_weekday', 'std_stay_weekend', \n",
    "                                    'centrality1K', 'rev_centrality1', 'rev_centrality5', 'n_pier', 'k_pier', 'd_pier',\n",
    "                                    'avg_leave_mov_length',\n",
    "                                   ], axis=1, inplace=False)\n",
    "\n",
    "    n_stop_weekday = (df_small_par[\"n_stop_weekday_day\"] + df_small_par[\"n_stop_weekday_night\"])/2\n",
    "    n_stop_weekend = (df_small_par[\"n_stop_weekend_day\"] + df_small_par[\"n_stop_weekend_night\"])/2\n",
    "\n",
    "    avg_time_weekday = (df_small_par[\"avg_time_weekday_day\"] + df_small_par[\"avg_time_weekday_night\"])/2\n",
    "    avg_time_weekend = (df_small_par[\"avg_time_weekend_day\"] + df_small_par[\"avg_time_weekend_night\"])/2\n",
    "\n",
    "    df_small_par = df_small_par.assign(n_stop_weekday=n_stop_weekday, n_stop_weekend=n_stop_weekend, \n",
    "                                       avg_arrival_weekday=avg_time_weekday, avg_arrival_weekend=avg_time_weekend)\n",
    "\n",
    "    df_small_par.drop([\"n_stop_weekday_day\", \"n_stop_weekday_night\", \"n_stop_weekend_day\", \"n_stop_weekend_night\",\n",
    "                      \"avg_time_weekday_day\", \"avg_time_weekday_night\", \"avg_time_weekend_day\", \"avg_time_weekend_night\"], axis=1, inplace=True)\n",
    "\n",
    "#     print(len(df_par.keys()))\n",
    "#     print(len(df_small_par.keys()))\n",
    "\n",
    "    # move the collective features as the last columns of the dataframe\n",
    "    columns_df_c = [\"exclusivity\", \"centrality15K\", \"rev_centrality20\"]\n",
    "    df_small_par = df_small_par[[c for c in df_small_par if c not in columns_df_c] + [c for c in columns_df_c if c in df_small_par]]\n",
    "\n",
    "    # move the geographical features as the last columns of the dataframe\n",
    "    categories = [\"gas\", \"parking\", \"hotel\", \"food\", \"leisure\", \"shop\", \"service\", \"supermarket\"]\n",
    "    columns_df_g = [\"n_\"+c for c in categories]+[\"k_\"+c for c in categories]+[\"d_\"+c for c in categories]\n",
    "    df_small_par = df_small_par[[c for c in df_small_par if c not in columns_df_g] + [c for c in columns_df_g if c in df_small_par]]\n",
    "\n",
    "    # move linkage column to the end\n",
    "    df_small_par = df_small_par[[c for c in df_small_par if c not in [\"link_cluster\"]] + [c for c in [\"link_cluster\"] if c in df_small_par]]\n",
    "\n",
    "    return df_small_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################# USING PANDAS LIBRARY ################ NEW VERSION\n",
    "# def draw_par_coords(df_par, link_cluster_to_n_location, cluster_id, colors, linestyle):\n",
    "#     # draw the parallel coordinates of the linkage clusters\n",
    "#     fig = plt.figure(figsize=(20, 10)) \n",
    "\n",
    "#     cmap = cm.gist_rainbow(np.linspace(0.1, 1, 7))\n",
    "\n",
    "#     parallel_coordinates(df_par, 'link_cluster', color = cmap, linewidth=3, linestyle=linestyle, axvlines=True, \\\n",
    "#                          axvlines_kwds={\"linewidth\":0.1, \"color\":\"k\"} )\n",
    "#     plt.xticks(rotation=90, fontsize=16)\n",
    "#     plt.yticks(np.arange(0, 1.01, 0.1), fontsize=16)\n",
    "\n",
    "#     legend_handles = []\n",
    "#     for i in range(1, len(link_cluster_to_n_location)+1):\n",
    "#         legend_handles.append(mpatches.Patch(color=cmap[(i-1)%7], label='C'+str(i)+', n_locs ='+str(link_cluster_to_n_location[i])))\n",
    "\n",
    "#     plt.legend(handles=legend_handles, loc=1)\n",
    "    \n",
    "#     plt.grid(False)\n",
    "    \n",
    "#     plt.text(3, -0.01, 'Individual Features')\n",
    "#     plt.text(0, -0.03, '----------------------------------------------------------------')\n",
    "#     plt.text(11.5, -0.01, 'Collective Features')\n",
    "#     plt.text(13, -0.03, '----------')\n",
    "#     plt.text(26, -0.01, 'Geographic Features')\n",
    "#     plt.text(16, -0.03, '-----------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "#     plt.text(15.8, -0.37, '(number of POIs in \\na radius of 500 meters)')\n",
    "#     plt.text(23.6, -0.37, '(number of POIs between \\nthe 30 nearest neighbors)')\n",
    "#     plt.text(33, -0.37, '(distance of the \\nclosest POI)')\n",
    "\n",
    "#     plt.savefig('../../../thesis/images/parallel_coord_'+id_area+'_cluster_'+str(cluster_id+2)+'.png', format='png', bbox_inches='tight')\n",
    "#     plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# USING PANDAS LIBRARY ################ NEWEST VERSION\n",
    "def draw_par_coords(df_total, lines_to_draw, colors_parall, parall_linestyle, cluster_id):\n",
    "        \n",
    "    # draw the parallel coordinates of the linkage clusters\n",
    "    fig, ax = plt.subplots(1,1, figsize=(16, 6)) \n",
    "    #fig = plt.figure(figsize=(16, 8)) \n",
    "    x = df_total.keys()[:-3] #[x + y for x, y in zip(range(l), np.ones(l)*0.05*i)]\n",
    "    \n",
    "    y=[]\n",
    "        \n",
    "    for i, c in enumerate(lines_to_draw):\n",
    "        \n",
    "        row = df_total[df_total[\"partial_cluster\"] == c].iloc[0]        \n",
    "        y.append(np.array(row[:-3], dtype=float))     \n",
    "        ax.plot(x, y[i], color=colors_parall[i], linewidth=4, linestyle=parall_linestyle[i])\n",
    "        \n",
    "    plt.xticks(rotation='vertical', fontsize=16)\n",
    "    plt.yticks(np.arange(0, 1.05, 0.2), fontsize=22)\n",
    "    \n",
    "    ax.fill_between(x, y[0], y[1], facecolor='#f2f2f2')\n",
    "\n",
    "    legend_handles = []\n",
    "    for i, c in enumerate(lines_to_draw):\n",
    "        row = df_total[df_total[\"partial_cluster\"] == c].iloc[0]\n",
    "        n_locs = row.loc[\"partial_cluster_size\"]\n",
    "        p = int(round((n_locs * 100) /tot_loc))\n",
    "        legend_handles.append(mpatches.Patch(color=colors_parall[i], label=f\"$C_{i+1} ({p}\\%)$\"))\n",
    "    \n",
    "    plt.legend(handles=legend_handles, loc=4, ncol=2)\n",
    "    \n",
    "    plt.axvline(x=12.5, c='#333333')\n",
    "    plt.axvline(x=15.5, c='#333333')\n",
    "\n",
    "    plt.text(0, 0.88, 'Indiv',fontsize=25)\n",
    "    plt.text(12.8, 0.88, 'Coll',fontsize=25)\n",
    "    plt.text(15.8, 0.88, 'Cont',fontsize=25)\n",
    "\n",
    "    plt.text(15.8, -0.6, '(number of POIs in \\na radius of 500 \\nmeters)')\n",
    "    plt.text(23.6, -0.6, '(number of POIs \\nbetween the 30 \\nnearest neighbors)')\n",
    "    plt.text(32, -0.6, '(distance of the \\nclosest POI)')\n",
    "    \n",
    "    plt.title(\"Inter-regional Area\",fontsize=28)## area 2\n",
    "    #plt.title(\"Urban Area\",fontsize=28) ## area 11\n",
    "\n",
    "    plt.savefig('../../../thesis/images/parallel_coord_'+str(id_area)+'_cluster_'+str(cluster_id+2)+'.png', format='png', bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the clusters splitting recursively according to the dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### AREA 2\n",
    "link_matrix = compute_linkage_matrix(centroids_kmeans)\n",
    "\n",
    "colors = [[\"#b15928\", \"#b2df8a\"], [\"#b15928\", \"#33a02c\", \"#a6cee3\"], [\"#b15928\", \"#33a02c\", \"#1f78b4\", \"#e7298a\"],\n",
    "         [\"#525252\", \"#ff7f00\", \"#33a02c\", \"#1f78b4\", \"#e7298a\"], \n",
    "         [\"#525252\", \"#ffff33\", \"#e31a1c\", \"#33a02c\", \"#1f78b4\", \"#e7298a\"]]\n",
    "\n",
    "colors_parall = [[\"#b15928\", \"#b2df8a\"], [\"#525252\", \"#ff7f00\"], [\"#ffff33\", \"#e31a1c\"],\n",
    "                 [\"#33a02c\", \"#a6cee3\"], [\"#1f78b4\", \"#e7298a\"]]\n",
    "\n",
    "parall_linestyle = [[\"-\", \"-\"], [\"-\", \"-\"], [\"-\", \"-\"], [\"-\", \"-\"], [\"-\", \"-\"]]\n",
    "\n",
    "partial_cluster = [[\"1_3\", \"4_6\"], [\"1_3\", \"4\", \"5_6\"], [\"1_3\", \"4\", \"5\", \"6\"], [\"1\", \"2_3\", \"4\", \"5\", \"\"], \n",
    "                   [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]]\n",
    "\n",
    "lines_to_draw = [[\"1_3\", \"4_6\"], [\"1\", \"2_3\"], [\"2\", \"3\"], [\"4\", \"5_6\"],  [\"5\", \"6\"]]\n",
    "\n",
    "partial_cluster_size = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### AREA 11\n",
    "link_matrix = compute_linkage_matrix(centroids_kmeans)\n",
    "\n",
    "colors = [[\"#b15928\", \"#b2df8a\"], [\"#b15928\", \"#33a02c\", \"#a6cee3\"], [\"#525252\", \"#ff7f00\", \"#33a02c\", \"#a6cee3\"],\n",
    "         [\"#525252\", \"#ffff33\", \"#e31a1c\", \"#33a02c\", \"#a6cee3\"], \n",
    "         [\"#525252\", \"#ffff33\", \"#e31a1c\", \"#33a02c\", \"#1f78b4\", \"#e7298a\"]]\n",
    "\n",
    "colors_parall = [[\"#b15928\", \"#b2df8a\"], [\"#525252\", \"#ff7f00\"], [\"#ffff33\", \"#e31a1c\"],\n",
    "                 [\"#33a02c\", \"#a6cee3\"], [\"#1f78b4\", \"#e7298a\"]]\n",
    "\n",
    "parall_linestyle = [[\"-\", \"-\"], [\"-\", \"-\"], [\"-\", \"-\"], [\"-\", \"-\"], [\"-\", \"-\"]]\n",
    "\n",
    "partial_cluster = [[\"1_3\", \"4_6\"], [\"1_3\", \"4\", \"5_6\"], [\"1\", \"2_3\", \"4\", \"5_6\"], [\"1\", \"2\", \"3\", \"4\", \"5_6\"], \n",
    "                   [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]]\n",
    "\n",
    "lines_to_draw = [[\"1_3\", \"4_6\"], [\"1\", \"2_3\"], [\"2\", \"3\"], [\"4\", \"5_6\"],  [\"5\", \"6\"]]\n",
    "\n",
    "partial_cluster_size = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mpl.rcParams[\"legend.framealpha\"] = 0.95\n",
    "mpl.rcParams[\"legend.facecolor\"] = 'white' \n",
    "mpl.rcParams[\"legend.edgecolor\"] = 'white'\n",
    "\n",
    "first = True\n",
    "\n",
    "for i in range(5):\n",
    "    cut_dist = link_matrix[-i-1][2] - 0.01\n",
    "    \n",
    "    linkage_labels, link_cluster_to_n_location, link_cluster_to_n_kcluster, kcluster_to_link_cluster = clusters_dict(\n",
    "                        link_matrix, centroids_kmeans, labels_kmeans, kmeans_cluster_size)\n",
    "        \n",
    "    draw_dendro(i, link_matrix, cut_dist, link_cluster_to_n_location, colors[i], partial_cluster[i])    \n",
    "    #print(silhouette_score(centroids_kmeans, linkage_labels))\n",
    "    \n",
    "    df_centroids, df_par, link_centroids_std = create_df_centroids(centroids_kmeans, kcluster_to_link_cluster)\n",
    "    \n",
    "    df_small_par = remove_feat(df_par)\n",
    "    \n",
    "    if first:\n",
    "        df_total = df_small_par.copy()\n",
    "        first = False\n",
    "    else:\n",
    "        df_total = df_total.append(df_small_par)\n",
    "    partial_cluster_size.extend(link_cluster_to_n_location.values())\n",
    "         \n",
    "df_total[\"partial_cluster\"] = [label for sublist in partial_cluster for label in sublist] # flaten the list of list\n",
    "df_total[\"partial_cluster_size\"] = partial_cluster_size\n",
    "\n",
    "for i in range(5):          \n",
    "    #draw_par_coords_error(df_par, link_centroids_std, link_cluster_to_n_location, i)\n",
    "    draw_par_coords(df_total, lines_to_draw[i], colors_parall[i], parall_linestyle[i], i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRAW THE HEATMAP OF THE LOCATIONS COMPOSING THE CLUSTERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use selenium to transform a hmtl map into a png image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = selenium.webdriver.PhantomJS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the dataframe not normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataframe not normalized\n",
    "path = '../../../datasets/out/Traj' + stop + 'min/'\n",
    "file_name_in = 'loc_feat_area'+id_area+'_month'+month_code+'_week'+ week + '_complete.csv'\n",
    "\n",
    "df = pd.read_csv(path+file_name_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_denorm = df[[\"loc_proto_lat\", \"loc_proto_lon\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the array of linkage cluster label for each location and assign the linkage cluster to the dataset of the locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_cluster = []\n",
    "for kmeans_label in labels_kmeans:\n",
    "    link_cluster.append(kcluster_to_link_cluster[kmeans_label])\n",
    "    \n",
    "df_locs = df_denorm.copy()\n",
    "df_locs[\"link_cluster\"] = link_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the linkage cluster label for each location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+\"link_cluster\"+file_name_out+'.pickle', 'wb') as fp:\n",
    "    df_link = df.copy()\n",
    "    df_link = df_link[['vehicle', 'loc_id']]\n",
    "    df_link[\"link_cluster\"] = link_cluster\n",
    "    pickle.dump(df_link, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+\"link_cluster\"+file_name_out+'.pickle', 'rb') as fp:\n",
    "    df_link = pickle.load(fp)\n",
    "    link_cluster = df_link[\"link_cluster\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(silhouette_score(df_corr, link_cluster)) ## area 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(silhouette_score(df_corr, link_cluster)) ## area 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract all the points of the locations of a linkage cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array with k (linkage) elements, each contains a list of points in that cluster\n",
    "link_points = []\n",
    "for i in range(1, len(np.unique(link_cluster))+1):\n",
    "    df_i = df_locs[df_locs[\"link_cluster\"] == i]\n",
    "    link_points.append([list(a) for a in zip(df_i[\"loc_proto_lat\"], df_i[\"loc_proto_lon\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw the heatmap for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [[\"#ffffff\", \"#a6a6a6\", \"#525252\"], [\"#ffffe6\",\"#ffff33\", \"#ff9933\"],[\"#fce8e9\", \"#ef7678\", \"#e31a1c\"],\n",
    "          [\"#ecfaeb\", \"#8dde87\", \"#33a02c\"], [\"#e9f4fb\", \"#7cbde9\", \"#1f78b4\"], [\"#fce8f3\", \"#f28cc1\", \"#e7298a\"]]\n",
    "\n",
    "for i in range(len(link_points)):\n",
    "    \n",
    "    cluster_id = i\n",
    "#     m = folium.Map(location=[38.15, 23.5], zoom_start=11) ## area 2\n",
    "    m = folium.Map(location=[38, 23.68], zoom_start=12) ## area 11\n",
    "\n",
    "    # Plot it on the map\n",
    "    HeatMap(link_points[cluster_id], gradient={.4: colors[i][0], .65: colors[i][1], 1: colors[i][2]}).add_to(m)\n",
    "\n",
    "#     folium.map.Marker([38.8, 22.9], ## area 11 [38.2, 23.68] ## area 2 [38.8, 22.9]\n",
    "#         icon=folium.features.DivIcon(icon_size=(500,40), icon_anchor=(0,0),\n",
    "#                                      html='<div style=\"font-size: 56pt\">CLUSTER '+str(cluster_id+1)+'</div>')).add_to(m)\n",
    "    # Display the map\n",
    "    m.save('../../../thesis/images/heatmap_area_'+id_area+'_cluster_'+str(cluster_id+1)+'.html')\n",
    "        \n",
    "#     driver.set_window_size(2800, 2000)## area 2\n",
    "    driver.set_window_size(2400, 1500)## area 11\n",
    "        \n",
    "    driver.get('../../../thesis/images/heatmap_area_'+id_area+'_cluster_'+str(cluster_id+1)+'.html')\n",
    "    driver.save_screenshot('../../../thesis/images/heatmap_area_'+id_area+'_cluster_'+str(cluster_id+1)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PURITY AND ENTROPY DISTRIBUTION OF ANNOTATED IMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df[[\"vehicle\", \"loc_id\", \"support\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a df with the support for each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_lin = df_result.copy()\n",
    "df_result_lin[\"link_cluster\"] = link_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to compute the entropy of the cluster distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(n_locs, tot_locs):\n",
    "    e = 0\n",
    "    for j in range(len(n_locs)):\n",
    "        if n_locs[j] == 0:\n",
    "            continue\n",
    "        p = n_locs[j] / tot_locs\n",
    "        e -= p * np.log2(p)\n",
    "    return e / np.log2(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get the list of vehicletypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.read_csv('../../../datasets/in/Traj'+stop+'min/area'+id_area+'_month'+month_code+'_week'+ week+'_stops.csv')\n",
    "vehicletypes = []\n",
    "v_list = df_temp[\"vehicle\"].unique()\n",
    "for v in v_list:\n",
    "    df_i = df_temp[df_temp[\"vehicle\"] == v]\n",
    "    vehicletypes.append(df_i[\"vehicletype\"].unique()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataframe containing the count of locations and points in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_array = []\n",
    "\n",
    "\n",
    "for i, v in enumerate(df_result_lin[\"vehicle\"].unique()):\n",
    "    # get the df of that vehicle only\n",
    "    row = [v]\n",
    "    df_v = df_result_lin[df_result_lin[\"vehicle\"] == v]\n",
    "    \n",
    "    # get the vehicletype\n",
    "    row.append(vehicletypes[i])\n",
    "    \n",
    "    # compute the number of location in each cluster\n",
    "    n_locs = []\n",
    "    for lc in range(1,7):\n",
    "        n_locs.append(len(df_v[df_v[\"link_cluster\"] == lc]))\n",
    "    row.extend(n_locs)\n",
    "    row.append(len(df_v))\n",
    "    # compute the purity of the locations distribution\n",
    "    pur_loc = np.max(n_locs)/len(df_v)\n",
    "    row.append(pur_loc)\n",
    "    # compute the entropy of the locations distribution\n",
    "    row.append(entropy(n_locs, len(df_v)))\n",
    "    \n",
    "    # compute the number of points in each cluster\n",
    "    n_p = []\n",
    "    for lc in range(1,7):\n",
    "        df_lc = df_v[df_v[\"link_cluster\"] == lc]\n",
    "        n_p.append(np.sum(df_lc[\"support\"]))\n",
    "    row.extend(n_p)\n",
    "    row.append(np.sum(n_p))\n",
    "    # compute the purity of the points distribution\n",
    "    pur_loc = np.max(n_p)/np.sum(n_p)\n",
    "    row.append(pur_loc)\n",
    "    # compute the entropy of the locations distribution\n",
    "    row.append(entropy(n_p, np.sum(n_p)))\n",
    "    \n",
    "    df_array.append(row)\n",
    "\n",
    "columns_name = [\"vehicle\", \"vehicletype\", \"n_loc_cluster_1\", \"n_loc_cluster_2\", \"n_loc_cluster_3\", \"n_loc_cluster_4\", \n",
    "                \"n_loc_cluster_5\", \"n_loc_cluster_6\", \"tot_loc\", \"purity_loc\", \"entropy_loc\",\n",
    "                \"n_p_cluster_1\", \"n_p_cluster_2\", \"n_p_cluster_3\", \"n_p_cluster_4\", \"n_p_cluster_5\", \n",
    "                \"n_p_cluster_6\",\"tot_p\", \"purity_p\", \"entropy_p\"]\n",
    "df_purity = pd.DataFrame(df_array, columns=columns_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_purity.to_csv(path+\"df_purity\"+file_name_out+'.csv', mode = \"w\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### draw the distribution of the purity and the entropy for the locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.size\"] = 16\n",
    "\n",
    "xlabels = [\"locations purity\", \"locations entropy\", \"Purity\", \"Entropy\",]\n",
    "pur_entrop = [\"purity_loc\", \"entropy_loc\", \"purity_p\", \"entropy_p\"]\n",
    "title = []\n",
    "\n",
    "for i, pe in enumerate(pur_entrop):\n",
    "    fig = plt.figure(figsize=(6, 4)) \n",
    "\n",
    "    y = sorted(df_purity[pe])\n",
    "    _, bins, _ = plt.hist(y, bins=np.arange(0,1.1, 0.1), color = '#0c2c84', ec='#FFFFFF')## area 11\n",
    "    #_, bins, _ = plt.hist(y, bins=np.arange(0,1.1, 0.1), color = '#A8201A', ec='#FFFFFF')## area 2\n",
    "    plt.xlabel(xlabels[i], fontsize=19)\n",
    "    plt.ylabel(\"number of vehicles\", fontsize=19)\n",
    "    \n",
    "    #plt.title(\"Inter-regional Area\")## area 2\n",
    "    plt.title(\"Urban Area\") ## area 11\n",
    "\n",
    "    plt.savefig('../../../thesis/images/results/'+pe+file_name_out+'.png', format='png', bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### draw the distribution of the purity and the entropy for the locations (STACKED over the vehicletype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.size\"] = 15\n",
    "\n",
    "xlabels = [\"locations purity\", \"locations entropy\", \"Purity\", \"Entropy\",]\n",
    "pur_entrop = [\"purity_loc\", \"entropy_loc\", \"purity_p\", \"entropy_p\"]\n",
    "colors = [\"#d53e4f\", \"#fdae61\", \"#fee08b\", \"#abdda4\", \"#3288bd\"]\n",
    "\n",
    "for i, pe in enumerate(pur_entrop):\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    \n",
    "    pile = []\n",
    "    for vt in set(vehicletypes):\n",
    "        pile.append(df_purity[df_purity['vehicletype'] == vt][pe])\n",
    "    \n",
    "    y = sorted(df_purity[pe])\n",
    "    _, bins, _ = plt.hist(pile, bins=np.arange(0,1.1, 0.1), color = colors[:len(set(vehicletypes))], stacked=True)\n",
    "    plt.xlabel(xlabels[i], fontsize=17)\n",
    "    plt.ylabel(\"number of vehicles\", fontsize=17)\n",
    "    \n",
    "    plt.legend(list(set(vehicletypes)), loc='best')\n",
    "    \n",
    "    #plt.title(\"Inter-regional Area\")## area 2\n",
    "    plt.title(\"Urban Area\") ## area 11\n",
    "\n",
    "    plt.savefig('../../../thesis/images/results/'+pe+\"_stacked_\"+file_name_out+'.png', format='png', bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cor_pur = df_purity.copy()\n",
    "\n",
    "diff_clust = []\n",
    "for i, r in df_cor_pur.iterrows():\n",
    "    npc = \"n_p_cluster_\"\n",
    "    n_c = 0\n",
    "    for j in range(1, 7):\n",
    "        if r[npc+str(j)] != 0:\n",
    "            n_c += 1\n",
    "    diff_clust.append(n_c)\n",
    "df_cor_pur[\"diff_clust\"] = diff_clust "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cor_pur.drop([\"vehicle\", \"vehicletype\", \"n_loc_cluster_1\", \"n_loc_cluster_2\", \"n_loc_cluster_3\", \"n_loc_cluster_4\", \n",
    "                \"n_loc_cluster_5\", \"n_loc_cluster_6\", \"tot_loc\", \"purity_loc\", \"entropy_loc\", \n",
    "                \"n_p_cluster_1\", \"n_p_cluster_2\", \"n_p_cluster_3\", \"n_p_cluster_4\", \"n_p_cluster_5\", \n",
    "                \"n_p_cluster_6\",\"tot_p\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cor_pur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color map from purple to orange\n",
    "cmap = cm.get_cmap('PuOr')\n",
    "\n",
    "plt.rcParams[\"font.size\"] = '16'\n",
    "\n",
    "# individual heatmap\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(10,8)\n",
    "\n",
    "# take only the first part of the dataset\n",
    "correlati = df_cor_pur.corr()\n",
    "correlati = correlati.round(2)\n",
    "ax = sns.heatmap(correlati, cmap=cmap, vmin = -1, vmax = 1, annot = True,linewidths=.4)\n",
    "\n",
    "plt.savefig('../../../thesis/images/corr.png', format='png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
